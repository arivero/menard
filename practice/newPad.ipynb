{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kampal/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kampal/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize teacher model normally in full precision\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(model_name, fast=False) # Fast=False ensures we get the correct tokenizer. IMPORTANT!  \n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"  # This will handle CUDA allocation efficiently\n",
    ")\n",
    "\n",
    "# Set padding token for the tokenizer\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    # Resize token embeddings for the model to account for the new token\n",
    "    teacher_model.resize_token_embeddings(len(teacher_tokenizer))\n",
    "\n",
    "# Configure LoRA to only train the adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    modules_to_save=None  # Don't save any full modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a checksum all the layers of teacher\n",
    "import hashlib\n",
    "def compute_model_checksums(model):\n",
    "    checksums = {}\n",
    "    for name, param in model.state_dict().items():\n",
    "        param_data = param.detach().cpu().numpy()\n",
    "        param_bytes = param_data.tobytes()\n",
    "        checksum = hashlib.md5(param_bytes).hexdigest()\n",
    "        checksums[name] = checksum\n",
    "    return checksums\n",
    "\n",
    "\n",
    "# 1. Compute and store initial checksums\n",
    "initial_checksums = compute_model_checksums(teacher_model)\n",
    "# 3. Compute checksums again after fine-tuning\n",
    "fine_tuned_checksums = compute_model_checksums(teacher_model)\n",
    "\n",
    "# 4. Compare initial and fine-tuned checksums\n",
    "def compare_checksums(initial, fine_tuned):\n",
    "    differences = {}\n",
    "    for layer_name in initial.keys():\n",
    "        if initial[layer_name] != fine_tuned[layer_name]:\n",
    "            differences[layer_name] = {\n",
    "                \"initial\": initial[layer_name],\n",
    "                \"fine_tuned\": fine_tuned[layer_name],\n",
    "            }\n",
    "    return differences\n",
    "\n",
    "\n",
    "checksum_differences = compare_checksums(initial_checksums, fine_tuned_checksums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No changes detected in the model's layers.\n"
     ]
    }
   ],
   "source": [
    "# Display differences\n",
    "if checksum_differences:\n",
    "    print(\"\\nLayers with changed checksums:\")\n",
    "    for layer, diff in checksum_differences.items():\n",
    "        print(f\"{layer}:\")\n",
    "        print(f\"  Initial: {diff['initial']}\")\n",
    "        print(f\"  Fine-tuned: {diff['fine_tuned']}\")\n",
    "else:\n",
    "    print(\"\\nNo changes detected in the model's layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=512):\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        padding_side=\"left\",\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length',\n",
    "        truncation=True, \n",
    "        max_length=max_length // 2  # Reduce input length to leave room for generation\n",
    "    ).to(device)  # Move inputs to GPU\n",
    "    print(inputs.attention_mask)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_length // 4,  # Allow generation of new tokens up to half max_length\n",
    "            temperature=None, #0.7,\n",
    "            top_p=None, #was 0.9\n",
    "            do_sample=False,\n",
    "            #we allow repetition:\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            #output_scores=True,\n",
    "            output_logits=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    # Convert tuple of tensors into a single tensor\n",
    "    logits_tensor = torch.cat([t.unsqueeze(1) for t in outputs.logits], dim=1)\n",
    "    new_logits = logits_tensor  # Replace tuple with tensor. This is the tensor of logits for the new tokens, shape: (batch_size, num_new_tokens, vocab_size)\n",
    "    new_tokens = outputs.sequences[:, inputs.input_ids.shape[-1]:]\n",
    "    old_tokens = inputs.input_ids\n",
    "    #print shapes\n",
    "    print(new_logits.shape, new_tokens.shape, old_tokens.shape, outputs.sequences.shape, len(outputs.logits))\n",
    "    #return tokenizer.decode(new_tokens[0], skip_special_tokens=True)\n",
    "    #print (outputs)\n",
    "    original=[tokenizer.decode(seq, skip_special_tokens=False) for seq in outputs.sequences]\n",
    "    print(original)\n",
    "    decoded=[tokenizer.decode(seq, skip_special_tokens=False) for seq in new_tokens]\n",
    "    return decoded, new_logits, original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return full text = False is an option in pipeline but not in generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 32001]) torch.Size([2, 16]) torch.Size([2, 32]) torch.Size([2, 48]) 16\n",
      "[\"[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> What is the capital of Spain?\\nWhat is Spain's currency?</s>[PAD][PAD][PAD][PAD][PAD][PAD][PAD]\", '[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Cual es la capital de Francia?\\nWhat is the capital of France? Paris is a city in France. It']\n"
     ]
    }
   ],
   "source": [
    "decoded,logits, original =generate_response(teacher_model, teacher_tokenizer, [\"What is the capital of Spain?\", \"Cual es la capital de Francia?\"],64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nWhat is Spain's currency?</s>[PAD][PAD][PAD][PAD][PAD][PAD][PAD]\",\n",
       " '\\nWhat is the capital of France? Paris is a city in France. It']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000,     1, 29871,  1724,   338,   278,  7483,\n",
      "           310, 13616, 29973,    13,  5618,   338, 13616, 29915, 29879, 27550,\n",
      "         29973,     2, 32000, 32000, 32000, 32000, 32000, 32000,   310,   278,\n",
      "         29871, 29896, 29929, 29929, 29900, 29879, 29889,    13,  1576, 29871,\n",
      "         29896, 29929, 29929, 29900, 29879,   892,   263,   316,  6332,   310,\n",
      "          1735, 29889,    13,  1576, 29871, 29896, 29929, 29929]],\n",
      "       device='cuda:0')\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  What is the capital of Spain?\n",
      "What is Spain's currency?</s>[PAD][PAD][PAD][PAD][PAD][PAD] of the 1990s.\n",
      "The 1990s were a decade of change.\n",
      "The 199\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = teacher_tokenizer(\n",
    "    original[0], \n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=False  # Do not add special tokens like <s> or </s>\n",
    ").to(device)\n",
    "padTokens=inputs.input_ids[inputs.input_ids == 32000].shape[0]\n",
    "tokens=inputs.input_ids[:8+padTokens].view(1,-1)\n",
    "inputs.attention_mask[inputs.input_ids == 32000]=0\n",
    "amask=inputs.attention_mask[:8+padTokens].view(1,-1).to(device)\n",
    "#cut some padding\n",
    "tokens=tokens[:,:-1]\n",
    "amask=amask[:,:-1]\n",
    "for i in range (30):\n",
    "    #run model for a next token\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(\n",
    "            input_ids=tokens,\n",
    "            attention_mask=amask\n",
    "        )\n",
    "    next_token=torch.argmax(outputs.logits[0,-1,:])\n",
    "    next_token = next_token.view(1, 1)\n",
    "    tokens=torch.cat((tokens,next_token),dim=1)\n",
    "    amask=torch.cat((amask,torch.ones(1,1).to(device)),dim=1)\n",
    "print(tokens)\n",
    "print(teacher_tokenizer.decode(tokens[0], skip_special_tokens=False))\n",
    "print(amask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000,     1, 29871,  1724,   338,   278,  7483,\n",
      "           310, 13616, 29973,    13,  5618,   338, 13616, 29915, 29879, 27550,\n",
      "         29973,     2, 32000, 32000, 32000, 32000, 32000, 32000, 32000,   310,\n",
      "           278,   310,   278,   310,   278,   310,   278,   310,   278,   310,\n",
      "           278,   310,   278,   310,   278,   310,   278,   310,   278,   310,\n",
      "           278,   310,   278,   310,   278,   310,   278,   310,   278]],\n",
      "       device='cuda:0')\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  What is the capital of Spain?\n",
      "What is Spain's currency?</s>[PAD][PAD][PAD][PAD][PAD][PAD][PAD] of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokens=inputs.input_ids[:8+padTokens].view(1,-1)\n",
    "inputs.attention_mask[inputs.input_ids == 32000]=0\n",
    "amask=inputs.attention_mask[:8+padTokens].view(1,-1).to(device)\n",
    "for i in range (30):\n",
    "    #run model for a next token\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(\n",
    "            input_ids=tokens,\n",
    "            attention_mask=amask\n",
    "        )\n",
    "    next_token=torch.argmax(outputs.logits[0,-1,:])\n",
    "    next_token = next_token.view(1, 1)\n",
    "    tokens=torch.cat((tokens,next_token),dim=1)\n",
    "    amask=torch.cat((amask,torch.ones(1,1).to(device)),dim=1)\n",
    "print(tokens)\n",
    "print(teacher_tokenizer.decode(tokens[0], skip_special_tokens=False))\n",
    "print(amask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000,     1, 29871,  1724,   338,   278,  7483,\n",
      "           310, 13616, 29973,    13,  5618,   338, 13616, 29915, 29879, 27550,\n",
      "         29973,     2, 32000, 32000, 32000, 32000, 32000, 32000, 32000,    13,\n",
      "         29892,   341, 29892,   341, 29889, 29943, 29889, 29889, 29889, 29889,\n",
      "         29896,  4345,  4345,  4345, 29892, 29896, 29889, 29943, 29879, 29924,\n",
      "         29924,  4345,  4345, 29896, 29889, 29892, 29924, 29924,  4345]],\n",
      "       device='cuda:0')\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  What is the capital of Spain?\n",
      "What is Spain's currency?</s>[PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      ", M, M.F....1MSMSMS,1.FsMMMSMS1.,MMMS\n"
     ]
    }
   ],
   "source": [
    "tokens=inputs.input_ids[:8+padTokens].view(1,-1)\n",
    "#tokens=tokens[:,:-5]\n",
    "for i in range (30):\n",
    "    #run model for a next token\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(\n",
    "            input_ids=tokens,\n",
    "        )\n",
    "    next_token=torch.argmax(outputs.logits[0,-1,:])\n",
    "    next_token = next_token.view(1, 1)\n",
    "    tokens=torch.cat((tokens,next_token),dim=1)\n",
    "    amask=torch.cat((amask,torch.ones(1,1).to(device)),dim=1)\n",
    "print(tokens)\n",
    "print(teacher_tokenizer.decode(tokens[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 49])\n",
      "tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000,     1, 29871,  1724,   338,   278,  7483,\n",
      "           310, 13616, 29973,    13,  5618,   338, 13616, 29915, 29879, 27550,\n",
      "         29973,     2, 32000, 32000, 32000, 32000, 32000, 32000, 32000]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0]], device='cuda:0')\n",
      "tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000, 32000,     1, 29871,  1724,   338,   278,  7483,\n",
      "           310, 13616, 29973,    13,  5618,   338, 13616, 29915, 29879, 27550,\n",
      "         29973,     2, 32000, 32000, 32000, 32000, 32000, 32000, 32000]],\n",
      "       device='cuda:0')\n",
      "Logits shape: torch.Size([1, 49, 32001])\n",
      "Decoded output: ..sssss.......... of of of of of of of #1 Is the purpose of the?\n",
      "What is the's currency?\n",
      "<s> of of.. of of of\n",
      "output tokens: tensor([29889, 29889, 29879, 29879, 29879, 29879, 29879, 29889, 29889, 29889,\n",
      "        29889, 29889, 29889, 29889, 29889, 29889, 29889,   310,   310,   310,\n",
      "          310,   310,   310,   310,   396, 29896,  1317,   278,  6437,   310,\n",
      "          278, 29973,    13,  5618,   338,   278, 29915, 29879, 27550, 29973,\n",
      "           13,     1,   310,   310, 29889, 29889,   310,   310,   310],\n",
      "       device='cuda:0')\n",
      "tensor([[    1, 29871,  1724,   338,   278,  7483,   310, 13616, 29973,    13,\n",
      "          5618,   338, 13616, 29915, 29879, 27550, 29973,     2,     1,   450,\n",
      "         29871, 29906, 29900, 29896, 29947, 29899, 29896, 29929,  4259,   338,\n",
      "           278, 29871, 29896, 29900,   386,  4259,   310,   278,  3086, 10152,\n",
      "         29915, 29879, 18993,  5165,   313, 29940,  7811, 29931]],\n",
      "       device='cuda:0')\n",
      "<s>  What is the capital of Spain?\n",
      "What is Spain's currency?</s><s> The 2018-19 season is the 10th season of the National Women's Soccer League (NWSL\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the first element of the decoded list\n",
    "inputs = teacher_tokenizer(\n",
    "    original[0], \n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=False  # Do not add special tokens like <s> or </s>\n",
    ").to(device)\n",
    "print(\"Input shape:\", inputs.input_ids.shape)\n",
    "print(inputs.input_ids)\n",
    "print(inputs.attention_mask)\n",
    "#wait, attention_mask must be zero for padding tokens, the ones that are 32000 in the input_ids:\n",
    "inputs.attention_mask[inputs.input_ids == 32000] = 0\n",
    "#inputs.attention_mask[inputs.input_ids == 1] = 0  # esto en cambio no debería ser necesario, porque indica de verdad el comienzo de la secuencia\n",
    "print(inputs.attention_mask)\n",
    "#reshape removing all the padding (32000)\n",
    "#inputs.input_ids=inputs.input_ids[inputs.input_ids != 32000]\n",
    "#inputs.input_ids=inputs.input_ids.view(1,-1)\n",
    "print(inputs.input_ids)\n",
    "# Forward pass through the teacher model\n",
    "with torch.no_grad():\n",
    "    outputs = teacher_model(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        labels=inputs.input_ids,  # For calculating loss\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "# Print the logits and the decoded output\n",
    "print(\"Logits shape:\", outputs.logits.shape)\n",
    "decoded_output = teacher_tokenizer.decode(torch.argmax(outputs.logits, dim=-1)[0], skip_special_tokens=False)\n",
    "print(\"Decoded output:\", decoded_output)\n",
    "print(\"output tokens:\", torch.argmax(outputs.logits, dim=-1)[0])\n",
    "#now we do the same but token to token.\n",
    "tokens=inputs.input_ids[inputs.input_ids != 32000].view(1,-1)\n",
    "for i in range (30):\n",
    "    #run model for a next token\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(\n",
    "            input_ids=tokens\n",
    "        )\n",
    "    #get the next token\n",
    "    next_token=torch.argmax(outputs.logits[0,-1,:])\n",
    "    #print the token and its decoded value\n",
    "    #print(next_token.item(), teacher_tokenizer.decode(next_token.item()))\n",
    "    #reshape next_token to match the dimensions of tokens\n",
    "    next_token = next_token.view(1, 1)\n",
    "    #append the token to the input tensor\n",
    "    tokens=torch.cat((tokens,next_token),dim=1)\n",
    "    #print the new tensor\n",
    "print(tokens)\n",
    "    #print the decoded tensor\n",
    "print(teacher_tokenizer.decode(tokens[0], skip_special_tokens=False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: Explain how a binary search works. \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 128, 32001]) torch.Size([1, 128]) torch.Size([1, 256]) torch.Size([1, 384]) 128\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain how a binary search works. \\n\\n Your Answer:\\nA binary tree is a tree data structure in which each node has at most two children. The binary-search algorithm is an efficient algorithm for finding an item in a sorted array. It works by repeatedly dividing the array in half until the desired item is found. This algorithm can be used to search for an element in an array, a list, or a set. Binary search is often used in computer science to find an exact match in sorted data. For example, it can find the index of a given element or the position of an object in the sorted list. In addition, binary searches can also be applied to other types of']\n",
      "Teacher response: \n",
      "['\\nA binary tree is a tree data structure in which each node has at most two children. The binary-search algorithm is an efficient algorithm for finding an item in a sorted array. It works by repeatedly dividing the array in half until the desired item is found. This algorithm can be used to search for an element in an array, a list, or a set. Binary search is often used in computer science to find an exact match in sorted data. For example, it can find the index of a given element or the position of an object in the sorted list. In addition, binary searches can also be applied to other types of']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 128, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 663\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: What is the difference between a list and tuple in Python? \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 86, 32001]) torch.Size([1, 86]) torch.Size([1, 256]) torch.Size([1, 342]) 86\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: What is the difference between a list and tuple in Python? \\n\\n Your Answer:\\nA list is a mutable sequence of objects, while a tuple is an immutable sequence.  A list can be changed, but a tuples cannot. A tuple can only be used to store values, not to perform operations on them. Tuples are often used for storing data that needs to be accessed in a specific order, such as a dictionary. Lists are more flexible and can store any type of data.</s>']\n",
      "Teacher response: \n",
      "['\\nA list is a mutable sequence of objects, while a tuple is an immutable sequence.  A list can be changed, but a tuples cannot. A tuple can only be used to store values, not to perform operations on them. Tuples are often used for storing data that needs to be accessed in a specific order, such as a dictionary. Lists are more flexible and can store any type of data.</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 86, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 458\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: How does garbage collection work in Python? \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 73, 32001]) torch.Size([1, 73]) torch.Size([1, 256]) torch.Size([1, 329]) 73\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: How does garbage collection work in Python? \\n\\n Your Answer:\\n 1. Garbage collection is a process in which the Python runtime automatically reclaims memory that is no longer in use. 2. The Python garbage collector is responsible for freeing up memory by identifying and recycling unused objects. This process is done automatically and is not something that the programmer needs to worry about.  \\n</s>']\n",
      "Teacher response: \n",
      "['\\n 1. Garbage collection is a process in which the Python runtime automatically reclaims memory that is no longer in use. 2. The Python garbage collector is responsible for freeing up memory by identifying and recycling unused objects. This process is done automatically and is not something that the programmer needs to worry about.  \\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 73, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 412\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: Explain the concept of decorators in Python. \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 128, 32001]) torch.Size([1, 128]) torch.Size([1, 256]) torch.Size([1, 384]) 128\n",
      "[\"[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain the concept of decorators in Python. \\n\\n Your Answer:\\n 1. Decorators are functions that are used to modify the behavior of other functions. They are often used in programming languages to add functionality to existing functions without having to rewrite them. In Python, decorator functions are written in the form of @function_name. The decorated function is passed as an argument to the decorating function. This allows the function to be modified without changing its signature. For example, the following code shows how to use a decoration function: 2. def decorate(func): 3. return func.__name__ + ' decor' 4. @decorate 5.\"]\n",
      "Teacher response: \n",
      "[\"\\n 1. Decorators are functions that are used to modify the behavior of other functions. They are often used in programming languages to add functionality to existing functions without having to rewrite them. In Python, decorator functions are written in the form of @function_name. The decorated function is passed as an argument to the decorating function. This allows the function to be modified without changing its signature. For example, the following code shows how to use a decoration function: 2. def decorate(func): 3. return func.__name__ + ' decor' 4. @decorate 5.\"]\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 128, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 647\n"
     ]
    }
   ],
   "source": [
    "# Define system prompt and tasks\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
    "Always format code properly and explain technical concepts clearly.\"\"\"\n",
    "\n",
    "tasks = [\n",
    "    \"Explain how a binary search works.\",\n",
    "    \"What is the difference between a list and tuple in Python?\",\n",
    "    \"How does garbage collection work in Python?\",\n",
    "    \"Explain the concept of decorators in Python.\",\n",
    "]\n",
    "#teacher_model=student_model\n",
    "def create_training_examples():\n",
    "    examples = []\n",
    "    for task in tasks:\n",
    "        full_prompt = f\"{SYSTEM_PROMPT}\\n\\nYour Task: {task} \\n\\n Your Answer:\"\n",
    "        student_prompt = f\"\\n\\nTask: {task} \\n\\n Your Answer:\"\n",
    "        # Get teacher's response\n",
    "        print(f\"Teacher prompt: \\n{full_prompt}\")\n",
    "        #with teacher_model.disable_adapter():\n",
    "        teacher_response, new_logits, _ = generate_response(teacher_model, teacher_tokenizer, full_prompt)\n",
    "        print (f\"Teacher response: \\n{teacher_response}\")\n",
    "        print(\"=====================================\")\n",
    "        examples.append({\n",
    "            \"prompt\": full_prompt,\n",
    "            \"student_prompt\": f\"\\n\\nTask: {task} \\n\\n Your Answer:\",\n",
    "            \"response_logits\": new_logits,\n",
    "            \"combined\": f\"{full_prompt}{teacher_response}\",\n",
    "            \"combined_student\": f\"{student_prompt}{teacher_response}\"\n",
    "        })\n",
    "        print(\"shape of new_logits\", new_logits.shape)\n",
    "        print(\"len of teacher response\", len(teacher_response))\n",
    "        print(\"len of combined_student\", len(f\"{student_prompt}{teacher_response}\"))\n",
    "    return examples\n",
    "\n",
    "examples= create_training_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student model using LoRA - this only creates adapter weights\n",
    "student_model = get_peft_model(teacher_model, lora_config)\n",
    "\n",
    "#student_model=teacher_model\n",
    "#student_model.add_adapter(lora_config)\n",
    "\n",
    "\n",
    "\n",
    "#teacher_model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters for LoRA adapters:\n",
      "trainable params: 4,194,304 || all params: 6,742,618,112 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "# Print only the trainable parameters (should be much smaller)\n",
    "print(\"Trainable parameters for LoRA adapters:\")\n",
    "student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 6,742,618,112 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "with student_model.disable_adapter():\n",
    "    student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,618,112 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with student_model.disable_adapter():\n",
    "   student_checksums=compute_model_checksums(student_model.base_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.22.self_attn.o_proj.weight True\n",
      "model.layers.22.mlp.gate_proj.weight True\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight True\n",
      "model.layers.22.post_attention_layernorm.weight True\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.23.self_attn.k_proj.weight True\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.23.self_attn.o_proj.weight True\n",
      "model.layers.23.mlp.gate_proj.weight True\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight True\n",
      "model.layers.23.post_attention_layernorm.weight True\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.24.self_attn.k_proj.weight True\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.24.self_attn.o_proj.weight True\n",
      "model.layers.24.mlp.gate_proj.weight True\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight True\n",
      "model.layers.24.post_attention_layernorm.weight True\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.25.self_attn.o_proj.weight True\n",
      "model.layers.25.mlp.gate_proj.weight True\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight True\n",
      "model.layers.25.post_attention_layernorm.weight True\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.26.self_attn.o_proj.weight True\n",
      "model.layers.26.mlp.gate_proj.weight True\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight True\n",
      "model.layers.26.post_attention_layernorm.weight True\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.27.self_attn.o_proj.weight True\n",
      "model.layers.27.mlp.gate_proj.weight True\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight True\n",
      "model.layers.27.post_attention_layernorm.weight True\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.28.self_attn.k_proj.weight True\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.28.self_attn.o_proj.weight True\n",
      "model.layers.28.mlp.gate_proj.weight True\n",
      "model.layers.28.mlp.up_proj.weight True\n",
      "model.layers.28.mlp.down_proj.weight True\n",
      "model.layers.28.input_layernorm.weight True\n",
      "model.layers.28.post_attention_layernorm.weight True\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.29.self_attn.k_proj.weight True\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.29.self_attn.o_proj.weight True\n",
      "model.layers.29.mlp.gate_proj.weight True\n",
      "model.layers.29.mlp.up_proj.weight True\n",
      "model.layers.29.mlp.down_proj.weight True\n",
      "model.layers.29.input_layernorm.weight True\n",
      "model.layers.29.post_attention_layernorm.weight True\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.30.self_attn.k_proj.weight True\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.30.self_attn.o_proj.weight True\n",
      "model.layers.30.mlp.gate_proj.weight True\n",
      "model.layers.30.mlp.up_proj.weight True\n",
      "model.layers.30.mlp.down_proj.weight True\n",
      "model.layers.30.input_layernorm.weight True\n",
      "model.layers.30.post_attention_layernorm.weight True\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.31.self_attn.k_proj.weight True\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.31.self_attn.o_proj.weight True\n",
      "model.layers.31.mlp.gate_proj.weight True\n",
      "model.layers.31.mlp.up_proj.weight True\n",
      "model.layers.31.mlp.down_proj.weight True\n",
      "model.layers.31.input_layernorm.weight True\n",
      "model.layers.31.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "for key in initial_checksums.keys(): \n",
    "    #replace termination .weight with .base_layer.weight\n",
    "    newkey=key.replace('.weight','.base_layer.weight')\n",
    "    #compare with key in student model if exists\n",
    "    if newkey in student_checksums.keys():\n",
    "        print(newkey, initial_checksums[key] == student_checksums[newkey])\n",
    "    elif key in student_checksums.keys():\n",
    "        print(key, initial_checksums[key] == student_checksums[key])\n",
    "    else:\n",
    "        print(f\"Key {newkey} not found in student model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 128, 32001]) torch.Size([1, 128]) torch.Size([1, 256]) torch.Size([1, 384]) 128\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> What is the capital of France?\\n Answer: Paris\\nWhat is a French fry? Answer : A potato cut into the shape of a stick\\nHow many countries are there in the world?  Answer  : 195\\nWhere is France located?   Answer   : In Europe\\nWho is Napoleon?    Answer    : The leader of the French army\\nWhy is Paris called the City of Light?     Answer     : Because it is\\nbright and beautiful\\nThe French flag has three colors. What are they?      Answer      : Blue,\\nwhite, and red\\nFrance is in Europe. Where is Europe?       Answer       : It is on']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Paris\\nWhat is a French fry? Answer : A potato cut into the shape of a stick\\nHow many countries are there in the world?  Answer  : 195\\nWhere is France located?   Answer   : In Europe\\nWho is Napoleon?    Answer    : The leader of the French army\\nWhy is Paris called the City of Light?     Answer     : Because it is\\nbright and beautiful\\nThe French flag has three colors. What are they?      Answer      : Blue,\\nwhite, and red\\nFrance is in Europe. Where is Europe?       Answer       : It is on'],\n",
       " tensor([[[-4.7355, -3.4162, 11.0670,  ..., -4.2172, -0.6887,  1.2191],\n",
       "          [ 1.7003,  2.7376, 15.6574,  ...,  0.0611, -0.3284,  2.7565],\n",
       "          [-9.7093, -8.5697,  2.1901,  ..., -7.9747, -4.8540, -4.1084],\n",
       "          ...,\n",
       "          [-6.2352, -7.4487, 10.2826,  ..., -3.4094, -3.1806, -0.1665],\n",
       "          [-2.5318, -2.8924, 12.1853,  ..., -0.0659, -2.8017,  1.4626],\n",
       "          [-4.4827, -5.0353, 12.1889,  ..., -2.8257, -2.6675,  0.7306]]],\n",
       "        device='cuda:0'),\n",
       " ['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> What is the capital of France?\\n Answer: Paris\\nWhat is a French fry? Answer : A potato cut into the shape of a stick\\nHow many countries are there in the world?  Answer  : 195\\nWhere is France located?   Answer   : In Europe\\nWho is Napoleon?    Answer    : The leader of the French army\\nWhy is Paris called the City of Light?     Answer     : Because it is\\nbright and beautiful\\nThe French flag has three colors. What are they?      Answer      : Blue,\\nwhite, and red\\nFrance is in Europe. Where is Europe?       Answer       : It is on'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(student_model, teacher_tokenizer, \"What is the capital of France?\\n Answer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 128, 32001]) torch.Size([1, 128]) torch.Size([1, 256]) torch.Size([1, 384]) 128\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> What is the capital of France?\\n Answer: Paris\\nWhat is a French fry? Answer : A potato cut into the shape of a stick\\nHow many countries are there in the world?  Answer  : 195\\nWhere is France located?   Answer   : In Europe\\nWho is Napoleon?    Answer    : The leader of the French army\\nWhy is Paris called the City of Light?     Answer     : Because it is\\nbright and beautiful\\nThe French flag has three colors. What are they?      Answer      : Blue,\\nwhite, and red\\nFrance is in Europe. Where is Europe?       Answer       : It is on']\n",
      "(['Paris\\nWhat is a French fry? Answer : A potato cut into the shape of a stick\\nHow many countries are there in the world?  Answer  : 195\\nWhere is France located?   Answer   : In Europe\\nWho is Napoleon?    Answer    : The leader of the French army\\nWhy is Paris called the City of Light?     Answer     : Because it is\\nbright and beautiful\\nThe French flag has three colors. What are they?      Answer      : Blue,\\nwhite, and red\\nFrance is in Europe. Where is Europe?       Answer       : It is on'], tensor([[[-4.7355, -3.4162, 11.0670,  ..., -4.2172, -0.6887,  1.2191],\n",
      "         [ 1.7003,  2.7376, 15.6574,  ...,  0.0611, -0.3284,  2.7565],\n",
      "         [-9.7093, -8.5697,  2.1901,  ..., -7.9747, -4.8540, -4.1084],\n",
      "         ...,\n",
      "         [-6.2352, -7.4487, 10.2826,  ..., -3.4094, -3.1806, -0.1665],\n",
      "         [-2.5318, -2.8924, 12.1853,  ..., -0.0659, -2.8017,  1.4626],\n",
      "         [-4.4827, -5.0353, 12.1889,  ..., -2.8257, -2.6675,  0.7306]]],\n",
      "       device='cuda:0'), ['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> What is the capital of France?\\n Answer: Paris\\nWhat is a French fry? Answer : A potato cut into the shape of a stick\\nHow many countries are there in the world?  Answer  : 195\\nWhere is France located?   Answer   : In Europe\\nWho is Napoleon?    Answer    : The leader of the French army\\nWhy is Paris called the City of Light?     Answer     : Because it is\\nbright and beautiful\\nThe French flag has three colors. What are they?      Answer      : Blue,\\nwhite, and red\\nFrance is in Europe. Where is Europe?       Answer       : It is on'])\n"
     ]
    }
   ],
   "source": [
    "with student_model.disable_adapter():\n",
    "    print(generate_response(student_model, teacher_tokenizer, \"What is the capital of France?\\n Answer:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= Dataset.from_list(examples)\n",
    "#for row in dataset:\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 151, 32001])\n",
      "student decoded '#1\\n#:\\nlain how to function search tree.\\n\\n\\n## task:\\nBinaryn\\\\ binary search is a tree data structure in which each node has at most two children. The simplest treesearch tree is an efficient algorithm for searching an element in a sorted array. The is by repeatedly dividing the array in half until the desired element is found. The algorithm is be used to search for an item in a array, a sorted, or a binary. The search is a used in computer science to find an element match in a data. It example, it can be the index of a given element in the position of a element in a sorted list. Binary computer, it search can be be used to other types of data\\n'\n",
      "teacher decoded '\\n\\n binary search is a tree data structure in which each node has at most two children.\\n binary searchsearch algorithm is an efficient algorithm for finding an item in a sorted array.\\n works by repeatedly dividing the array in half until the desired item is found.\\n algorithm is be used to search for an item in a array, a sorted, or a tree.\\n search is a used in computer science to find an element match in a data.\\n example, it can be the index of a given element in the position of a element in a sorted array.\\n addition, binary search can be be used to other types of'\n",
      "student argmax tensor([  396, 29896,    13, 29937, 29901,    13,  7420,   920,   304,   740,\n",
      "         2740,  5447, 29889,    13,    13,    13,  2277,  3414, 29901,    13,\n",
      "        25196, 29876, 29905,  7581,  2740,   338,   263,  5447,   848,  3829,\n",
      "          297,   607,  1269,  2943,   756,   472,  1556,  1023,  4344, 29889,\n",
      "          450, 20393,  5447,  4478,  5447,   338,   385,  8543,  5687,   363,\n",
      "        11975,   385,  1543,   297,   263, 12705,  1409, 29889,   450,   338,\n",
      "          491, 28424,  1933,  4821,   278,  1409,   297,  4203,  2745,   278,\n",
      "         7429,  1543,   338,  1476, 29889,   450,  5687,   338,   367,  1304,\n",
      "          304,  2740,   363,   385,  2944,   297,   263,  1409, 29892,   263,\n",
      "        12705, 29892,   470,   263,  7581, 29889,   450,  2740,   338,   263,\n",
      "         1304,   297,  6601, 10466,   304,  1284,   385,  1543,  1993,   297,\n",
      "          263,   848, 29889,   739,  1342, 29892,   372,   508,   367,   278,\n",
      "         2380,   310,   263,  2183,  1543,   297,   278,  2602,   310,   263,\n",
      "         1543,   297,   263, 12705,  1051, 29889, 29479,  6601, 29892,   372,\n",
      "         2740,   508,   367,   367,  1304,   304,   916,  4072,   310,   848,\n",
      "           13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  7581,  2740,   338,   263,  5447,   848,  3829,   297,\n",
      "          607,  1269,  2943,   756,   472,  1556,  1023,  4344, 29889,    13,\n",
      "         7581,  2740,  4478,  5687,   338,   385,  8543,  5687,   363,  9138,\n",
      "          385,  2944,   297,   263, 12705,  1409, 29889,    13,  1736,   491,\n",
      "        28424,  1933,  4821,   278,  1409,   297,  4203,  2745,   278,  7429,\n",
      "         2944,   338,  1476, 29889,    13,  5687,   338,   367,  1304,   304,\n",
      "         2740,   363,   385,  2944,   297,   263,  1409, 29892,   263, 12705,\n",
      "        29892,   470,   263,  5447, 29889,    13,  2740,   338,   263,  1304,\n",
      "          297,  6601, 10466,   304,  1284,   385,  1543,  1993,   297,   263,\n",
      "          848, 29889,    13,  1342, 29892,   372,   508,   367,   278,  2380,\n",
      "          310,   263,  2183,  1543,   297,   278,  2602,   310,   263,  1543,\n",
      "          297,   263, 12705,  1409, 29889,    13,  6124, 29892,  7581,  2740,\n",
      "          508,   367,   367,  1304,   304,   916,  4072,   310],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   920,   263,\n",
      "          7581,  2740,  1736, 29889, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29909,  7581,  5447,   338,   263,  5447,   848,\n",
      "          3829,   297,   607,  1269,  2943,   756,   472,  1556,  1023,  4344,\n",
      "         29889,   450,  7581, 29899,  4478,  5687,   338,   385,  8543,  5687,\n",
      "           363,  9138,   385,  2944,   297,   263, 12705,  1409, 29889,   739,\n",
      "          1736,   491, 28424,  1933,  4821,   278,  1409,   297,  4203,  2745,\n",
      "           278,  7429,  2944,   338,  1476, 29889,   910,  5687,   508,   367,\n",
      "          1304,   304,  2740,   363,   385,  1543,   297,   385,  1409, 29892,\n",
      "           263,  1051, 29892,   470,   263,   731, 29889, 29479,  2740,   338,\n",
      "          4049,  1304,   297,  6601, 10466,   304,  1284,   385,  2684,  1993,\n",
      "           297, 12705,   848, 29889,  1152,  1342, 29892,   372,   508,  1284,\n",
      "           278,  2380,   310,   263,  2183,  1543,   470,   278,  2602,   310,\n",
      "           385,  1203,   297,   278, 12705,  1051, 29889,   512,  6124, 29892,\n",
      "          7581, 29645,   508,   884,   367,  7436,   304,   916,  4072,   310,\n",
      "          2033]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: Explain how a binary search works. \\n\\n Your Answer:['\\\\nA binary tree is a tree data structure in which each node has at most two children. The binary-search algorithm is an efficient algorithm for finding an item in a sorted array. It works by repeatedly dividing the array in half until the desired item is found. This algorithm can be used to search for an element in an array, a list, or a set. Binary search is often used in computer science to find an exact match in sorted data. For example, it can find the index of a given element or the position of an object in the sorted list. In addition, binary searches can also be applied to other types of']\"\n",
      "input_ids torch.Size([1, 151])\n",
      "attention mask torch.Size([1, 151])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 86, 32001])\n",
      "student_outputs torch.Size([1, 114, 32001])\n",
      "student decoded \"#1\\n#:\\n is the difference between the ` and a? Python?\\n\\n\\n## task:\\nan', list is a mutable sequence type objects. while a tuple is an immutable sequence of',\\n list can be changed, while a tupleples cannot.  list is be be created to store values, while to store operations. them. ples are faster used to storing data that is to be accessed in a specific order. such as a list. s are often flexible and can be any type of data, <s>use\\n\"\n",
      "teacher decoded '\\n\\n list is a mutable sequence of objects, while a tuple is an immutable sequence of\\n\\n list can be changed, while a tupleples cannot.  list is be be used to store values, while to perform operations on them.\\nples are often used to storing data that needs to be accessed in a specific order, such as a list.\\ns are often flexible and can be any type of data.\\n'\n",
      "student argmax tensor([  396, 29896,    13, 29937, 29901,    13,   338,   278,  4328,  1546,\n",
      "          278,   421,   322,   263, 29973,  5132, 29973,    13,    13,    13,\n",
      "         2277,  3414, 29901,    13, 29874, 29876,   742,  1051,   338,   263,\n",
      "        26691,  5665,  1134,  3618, 29889,  1550,   263, 18761,   338,   385,\n",
      "         5198,  9246,  5665,   310,   742,    13,  1051,   508,   367,  3939,\n",
      "        29892,  1550,   263, 18761,  2701,  2609, 29889, 29871,  1051,   338,\n",
      "          367,   367,  2825,   304,  3787,  1819, 29892,  1550,   304,  3787,\n",
      "         6931, 29889,   963, 29889, 29871,  2701,   526,  8473,  1304,   304,\n",
      "        15446,   848,   393,   338,   304,   367, 20592,   297,   263,  2702,\n",
      "         1797, 29889,  1316,   408,   263,  1051, 29889, 29871, 29879,   526,\n",
      "         4049, 25706,   322,   508,   367,   738,  1134,   310,   848, 29892,\n",
      "        29871,     1,  1509,    13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  1051,   338,   263, 26691,  5665,   310,  3618, 29892,\n",
      "         1550,   263, 18761,   338,   385,  5198,  9246,  5665,   310,    13,\n",
      "           13,  1051,   508,   367,  3939, 29892,  1550,   263, 18761,  2701,\n",
      "         2609, 29889, 29871,  1051,   338,   367,   367,  1304,   304,  3787,\n",
      "         1819, 29892,  1550,   304,  2189,  6931,   373,   963, 29889,    13,\n",
      "         2701,   526,  4049,  1304,   304, 15446,   848,   393,  4225,   304,\n",
      "          367, 20592,   297,   263,  2702,  1797, 29892,  1316,   408,   263,\n",
      "         1051, 29889,    13, 29879,   526,  4049, 25706,   322,   508,   367,\n",
      "          738,  1134,   310,   848, 29889,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1724,   338,   278,  4328,\n",
      "          1546,   263,  1051,   322, 18761,   297,  5132, 29973, 29871,    13,\n",
      "            13,  3575,   673, 29901,  1839, 29905, 29876, 29909,  1051,   338,\n",
      "           263, 26691,  5665,   310,  3618, 29892,  1550,   263, 18761,   338,\n",
      "           385,  5198,  9246,  5665, 29889, 29871,   319,  1051,   508,   367,\n",
      "          3939, 29892,   541,   263,  5291,  2701,  2609, 29889,   319, 18761,\n",
      "           508,   871,   367,  1304,   304,  3787,  1819, 29892,   451,   304,\n",
      "          2189,  6931,   373,   963, 29889, 12603,  2701,   526,  4049,  1304,\n",
      "           363, 15446,   848,   393,  4225,   304,   367, 20592,   297,   263,\n",
      "          2702,  1797, 29892,  1316,   408,   263,  8600, 29889,  2391, 29879,\n",
      "           526,   901, 25706,   322,   508,  3787,   738,  1134,   310,   848,\n",
      "         29889,     2,   525, 29962]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: What is the difference between a list and tuple in Python? \\n\\n Your Answer:['\\\\nA list is a mutable sequence of objects, while a tuple is an immutable sequence.  A list can be changed, but a tuples cannot. A tuple can only be used to store values, not to perform operations on them. Tuples are often used for storing data that needs to be accessed in a specific order, such as a dictionary. Lists are more flexible and can store any type of data.</s> ']\"\n",
      "input_ids torch.Size([1, 114])\n",
      "attention mask torch.Size([1, 114])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "teacher_logits torch.Size([1, 73, 32001])\n",
      "student_outputs torch.Size([1, 98, 32001])\n",
      "student decoded '#1\\n#:\\n to the collection work in Java?\\n\\n\\n## task: Gn\\\\1. Pythonbage collection is automatic process that Python the Python garbage automatically reclaims memory that is no longer in use.\\\\ \\n. The Python runtime collector is a for freeing up memory that identifying and reclaimcling unused objects.  process is done automatically and is not visible that the programmer needs to worry about. 3n <s>use\\n'\n",
      "teacher decoded '\\n\\n\\n. Garbage collection is a process in Python the Python runtime automatically reclaims memory that is no longer in use.\\n\\n. The Python runtime collector is responsible for freeing up memory that identifying and reclaimcling unused objects.  process is done automatically and is not something that the programmer needs to worry about. \\n\\n'\n",
      "student argmax tensor([  396, 29896,    13, 29937, 29901,    13,   304,   278,  4333,   664,\n",
      "          297,  3355, 29973,    13,    13,    13,  2277,  3414, 29901, 29871,\n",
      "        29954, 29876, 29905, 29896, 29889,  5132, 17807,  4333,   338, 18428,\n",
      "         1889,   393,  5132,   278,  5132, 25861,  6336,  1162,  8342, 29879,\n",
      "         3370,   393,   338,   694,  5520,   297,   671,  7790, 29871,    13,\n",
      "        29889,   450,  5132, 10073,  6314,   272,   338,   263,   363,  3889,\n",
      "          292,   701,  3370,   393,  2893,  9215,   322,  1162,  8342, 19914,\n",
      "          443,  3880,  3618, 29889, 29871,  1889,   338,  2309,  6336,   322,\n",
      "          338,   451,  7962,   393,   278, 27922,  4225,   304, 15982,  1048,\n",
      "        29889, 29871, 29941, 29876, 29871,     1,  1509,    13],\n",
      "       device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  7455, 17807,  4333,   338,   263,  1889,\n",
      "          297,  5132,   278,  5132, 10073,  6336,  1162,  8342, 29879,  3370,\n",
      "          393,   338,   694,  5520,   297,   671, 29889,    13,    13, 29889,\n",
      "          450,  5132, 10073,  6314,   272,   338, 14040,   363,  3889,   292,\n",
      "          701,  3370,   393,  2893,  9215,   322,  1162,  8342, 19914,   443,\n",
      "         3880,  3618, 29889, 29871,  1889,   338,  2309,  6336,   322,   338,\n",
      "          451,  1554,   393,   278, 27922,  4225,   304, 15982,  1048, 29889,\n",
      "        29871,    13,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1128,   947, 25861,  4333,\n",
      "           664,   297,  5132, 29973, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29871, 29896, 29889,  7455, 17807,  4333,   338,\n",
      "           263,  1889,   297,   607,   278,  5132, 10073,  6336,  1162,  8342,\n",
      "         29879,  3370,   393,   338,   694,  5520,   297,   671, 29889, 29871,\n",
      "         29906, 29889,   450,  5132, 25861,  6314,   272,   338, 14040,   363,\n",
      "          3889,   292,   701,  3370,   491,  2893,  9215,   322,  1162, 29891,\n",
      "         19914,   443,  3880,  3618, 29889,   910,  1889,   338,  2309,  6336,\n",
      "           322,   338,   451,  1554,   393,   278, 27922,  4225,   304, 15982,\n",
      "          1048, 29889, 29871,   320, 29876,     2,   525, 29962]],\n",
      "       device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: How does garbage collection work in Python? \\n\\n Your Answer:['\\\\n 1. Garbage collection is a process in which the Python runtime automatically reclaims memory that is no longer in use. 2. The Python garbage collector is responsible for freeing up memory by identifying and recycling unused objects. This process is done automatically and is not something that the programmer needs to worry about.  \\\\n</s> ']\"\n",
      "input_ids torch.Size([1, 98])\n",
      "attention mask torch.Size([1, 98])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 153, 32001])\n",
      "student decoded \"#1\\n#:\\nlain how difference of aators in Python.\\n\\n\\n## task:\\nDecn\\\\ Dec. Decorators are functions that take used to modify the behavior of another functions. \\\\ are used used to Python to to add functionality to existing functions without having to rewrite them. \\\\ Python, decorators functions are used in the form of @function_name. \\\\ decorator function is called as an argument to the decorator function. The allows the decor to be modified without having its signature. Dec example, the following code shows how to use a decorator function to\\n\\n. Dec decorator(func): 3. def func.__name__ + '()ated 4. @decorate 5.](\\n\"\n",
      "teacher decoded \"\\n\\n\\n. Decorators are a that are used to modify the behavior of other functions.\\n are used used to Python languages to add functionality to existing functions. having to rewrite them.\\n Python, decorators functions are used in the form of @function_name.\\n decorator function is passed as an argument to the decorator function.\\n allows the decor to be modified without changing its signature.\\n example, the following code shows how to use a decorator function to\\n\\n. In decorator(func): 3. def func.__name__ + ' decorated 4. @decorate 5.\"\n",
      "student argmax tensor([  396, 29896,    13, 29937, 29901,    13,  7420,   920,  4328,   310,\n",
      "          263,  4097,   297,  5132, 29889,    13,    13,    13,  2277,  3414,\n",
      "        29901,    13,  6185, 29876, 29905,  3826, 29889,  3826,   272,  4097,\n",
      "          526,  3168,   393,  2125,  1304,   304,  6623,   278,  6030,   310,\n",
      "         1790,  3168, 29889,   320,   526,  1304,  1304,   304,  5132,   304,\n",
      "          304,   788,  9863,   304,  5923,  3168,  1728,  2534,   304, 10683,\n",
      "          963, 29889,   320,  5132, 29892, 10200,  4097,  3168,   526,  1304,\n",
      "          297,   278,   883,   310,   732,  2220, 29918,   978, 29889,   320,\n",
      "        10200,  1061,   740,   338,  2000,   408,   385,  2980,   304,   278,\n",
      "        10200,  1061,   740, 29889,   450,  6511,   278, 10200,   304,   367,\n",
      "         9120,  1728,  2534,   967, 12608, 29889,  3826,  1342, 29892,   278,\n",
      "         1494,   775,  3697,   920,   304,   671,   263, 10200,  1061,   740,\n",
      "          304,    13,    13, 29889,  3826, 10200,  1061, 29898,  9891,  1125,\n",
      "        29871, 29941, 29889,   822,  3653, 17255,   978,  1649,   718,   525,\n",
      "          580,   630, 29871, 29946, 29889,   732, 19557,   403, 29871, 29945,\n",
      "        29889,   850,    13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  3826,   272,  4097,   526,   263,   393,\n",
      "          526,  1304,   304,  6623,   278,  6030,   310,   916,  3168, 29889,\n",
      "           13,   526,  1304,  1304,   304,  5132, 10276,   304,   788,  9863,\n",
      "          304,  5923,  3168, 29889,  2534,   304, 10683,   963, 29889,    13,\n",
      "         5132, 29892, 10200,  4097,  3168,   526,  1304,   297,   278,   883,\n",
      "          310,   732,  2220, 29918,   978, 29889,    13, 10200,  1061,   740,\n",
      "          338,  4502,   408,   385,  2980,   304,   278, 10200,  1061,   740,\n",
      "        29889,    13,  6511,   278, 10200,   304,   367,  9120,  1728,  6480,\n",
      "          967, 12608, 29889,    13,  1342, 29892,   278,  1494,   775,  3697,\n",
      "          920,   304,   671,   263, 10200,  1061,   740,   304,    13,    13,\n",
      "        29889,   512, 10200,  1061, 29898,  9891,  1125, 29871, 29941, 29889,\n",
      "          822,  3653, 17255,   978,  1649,   718,   525, 10200,   630, 29871,\n",
      "        29946, 29889,   732, 19557,   403, 29871, 29945, 29889],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   278,  6964,\n",
      "           310, 10200,  4097,   297,  5132, 29889, 29871,    13,    13,  3575,\n",
      "           673, 29901,  3366, 29905, 29876, 29871, 29896, 29889,  3826,   272,\n",
      "          4097,   526,  3168,   393,   526,  1304,   304,  6623,   278,  6030,\n",
      "           310,   916,  3168, 29889,  2688,   526,  4049,  1304,   297,  8720,\n",
      "         10276,   304,   788,  9863,   304,  5923,  3168,  1728,  2534,   304,\n",
      "         10683,   963, 29889,   512,  5132, 29892, 10200,  1061,  3168,   526,\n",
      "          3971,   297,   278,   883,   310,   732,  2220, 29918,   978, 29889,\n",
      "           450, 10200,   630,   740,   338,  4502,   408,   385,  2980,   304,\n",
      "           278, 10200,  1218,   740, 29889,   910,  6511,   278,   740,   304,\n",
      "           367,  9120,  1728,  6480,   967, 12608, 29889,  1152,  1342, 29892,\n",
      "           278,  1494,   775,  3697,   920,   304,   671,   263, 10200,   362,\n",
      "           740, 29901, 29871, 29906, 29889,   822, 10200,   403, 29898,  9891,\n",
      "          1125, 29871, 29941, 29889,   736,  3653, 17255,   978,  1649,   718,\n",
      "           525, 10200, 29915, 29871, 29946, 29889,   732, 19557,   403, 29871,\n",
      "         29945,  1213, 29962]], device='cuda:0')\n",
      "input_ids decoded '<s> \\n\\nTask: Explain the concept of decorators in Python. \\n\\n Your Answer:[\"\\\\n 1. Decorators are functions that are used to modify the behavior of other functions. They are often used in programming languages to add functionality to existing functions without having to rewrite them. In Python, decorator functions are written in the form of @function_name. The decorated function is passed as an argument to the decorating function. This allows the function to be modified without changing its signature. For example, the following code shows how to use a decoration function: 2. def decorate(func): 3. return func.__name__ + \\' decor\\' 4. @decorate 5.\"]'\n",
      "input_ids torch.Size([1, 153])\n",
      "attention mask torch.Size([1, 153])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 151, 32001])\n",
      "student decoded '#1\\n\\n =\\nlain the to function search tree.\\n\\n\\n\\n task:\\n\\nn\\\\\\\\ search. a binary in structure that which the node\\n a most \\n.\\n\\n treesearch tree is a algorithm\\n for\\n the element in a binary..\\n is by binary\\niding the array into the. the array item is found.\\n is is be used to search for an item in a array.  binary of a a binary.\\n search is a used to a science to search the element element in a..\\n example, the is be the element of the in element in the element of a element in a array array.\\n the, the search are be be used to the data of data\\n'\n",
      "teacher decoded '\\n\\n binary search is a tree data structure in which each node has at most two children.\\n binary searchsearch algorithm is an efficient algorithm for finding an item in a sorted array.\\n works by repeatedly dividing the array in half until the desired item is found.\\n algorithm is be used to search for an item in a array, a sorted, or a tree.\\n search is a used in computer science to find an element match in a data.\\n example, it can be the index of a given element in the position of a element in a sorted array.\\n addition, binary search can be be used to other types of'\n",
      "student argmax tensor([  396, 29896,    13,    13,   353,    13,  7420,   278,   304,   740,\n",
      "         2740,  5447, 29889,    13,    13,    13,    13,  3414, 29901,    13,\n",
      "           13, 29876, 29905, 29905,  2740, 29889,   263,  7581,   297,  3829,\n",
      "          393,   607,   278,  2943,    13,   263,  1556, 29871,    13, 29889,\n",
      "           13,    13,  5447,  4478,  5447,   338,   263,  5687,    13,   363,\n",
      "           13,   278,  1543,   297,   263,  7581, 29889, 29889,    13,   338,\n",
      "          491,  7581,    13,  4821,   278,  1409,   964,   278, 29889,   278,\n",
      "         1409,  2944,   338,  1476, 29889,    13,   338,   338,   367,  1304,\n",
      "          304,  2740,   363,   385,  2944,   297,   263,  1409, 29889, 29871,\n",
      "         7581,   310,   263,   263,  7581, 29889,    13,  2740,   338,   263,\n",
      "         1304,   304,   263, 10466,   304,  2740,   278,  1543,  1543,   297,\n",
      "          263, 29889, 29889,    13,  1342, 29892,   278,   338,   367,   278,\n",
      "         1543,   310,   278,   297,  1543,   297,   278,  1543,   310,   263,\n",
      "         1543,   297,   263,  1409,  1409, 29889,    13,   278, 29892,   278,\n",
      "         2740,   526,   367,   367,  1304,   304,   278,   848,   310,   848,\n",
      "           13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  7581,  2740,   338,   263,  5447,   848,  3829,   297,\n",
      "          607,  1269,  2943,   756,   472,  1556,  1023,  4344, 29889,    13,\n",
      "         7581,  2740,  4478,  5687,   338,   385,  8543,  5687,   363,  9138,\n",
      "          385,  2944,   297,   263, 12705,  1409, 29889,    13,  1736,   491,\n",
      "        28424,  1933,  4821,   278,  1409,   297,  4203,  2745,   278,  7429,\n",
      "         2944,   338,  1476, 29889,    13,  5687,   338,   367,  1304,   304,\n",
      "         2740,   363,   385,  2944,   297,   263,  1409, 29892,   263, 12705,\n",
      "        29892,   470,   263,  5447, 29889,    13,  2740,   338,   263,  1304,\n",
      "          297,  6601, 10466,   304,  1284,   385,  1543,  1993,   297,   263,\n",
      "          848, 29889,    13,  1342, 29892,   372,   508,   367,   278,  2380,\n",
      "          310,   263,  2183,  1543,   297,   278,  2602,   310,   263,  1543,\n",
      "          297,   263, 12705,  1409, 29889,    13,  6124, 29892,  7581,  2740,\n",
      "          508,   367,   367,  1304,   304,   916,  4072,   310],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   920,   263,\n",
      "          7581,  2740,  1736, 29889, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29909,  7581,  5447,   338,   263,  5447,   848,\n",
      "          3829,   297,   607,  1269,  2943,   756,   472,  1556,  1023,  4344,\n",
      "         29889,   450,  7581, 29899,  4478,  5687,   338,   385,  8543,  5687,\n",
      "           363,  9138,   385,  2944,   297,   263, 12705,  1409, 29889,   739,\n",
      "          1736,   491, 28424,  1933,  4821,   278,  1409,   297,  4203,  2745,\n",
      "           278,  7429,  2944,   338,  1476, 29889,   910,  5687,   508,   367,\n",
      "          1304,   304,  2740,   363,   385,  1543,   297,   385,  1409, 29892,\n",
      "           263,  1051, 29892,   470,   263,   731, 29889, 29479,  2740,   338,\n",
      "          4049,  1304,   297,  6601, 10466,   304,  1284,   385,  2684,  1993,\n",
      "           297, 12705,   848, 29889,  1152,  1342, 29892,   372,   508,  1284,\n",
      "           278,  2380,   310,   263,  2183,  1543,   470,   278,  2602,   310,\n",
      "           385,  1203,   297,   278, 12705,  1051, 29889,   512,  6124, 29892,\n",
      "          7581, 29645,   508,   884,   367,  7436,   304,   916,  4072,   310,\n",
      "          2033]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: Explain how a binary search works. \\n\\n Your Answer:['\\\\nA binary tree is a tree data structure in which each node has at most two children. The binary-search algorithm is an efficient algorithm for finding an item in a sorted array. It works by repeatedly dividing the array in half until the desired item is found. This algorithm can be used to search for an element in an array, a list, or a set. Binary search is often used in computer science to find an exact match in sorted data. For example, it can find the index of a given element or the position of an object in the sorted list. In addition, binary searches can also be applied to other types of']\"\n",
      "input_ids torch.Size([1, 151])\n",
      "attention mask torch.Size([1, 151])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 86, 32001])\n",
      "student_outputs torch.Size([1, 114, 32001])\n",
      "student decoded \"#1 \\n =\\n is the sum between the and and a? Python?\\n\\n\\n\\n task:\\nan\\\\' is a list a of of.\\n a\\n is a\\nutable\\n of\\n\\n list is\\n\\n.\\n a listples..  list is be be\\n to\\n a.  a \\n. the.\\nples.\\n used to... is to be.. a. order.\\n as a list.\\ns.\\n\\n.  be. type of data.\\n<s>use\\n\"\n",
      "teacher decoded '\\n\\n list is a mutable sequence of objects, while a tuple is an immutable sequence of\\n\\n list can be changed, while a tupleples cannot.  list is be be used to store values, while to perform operations on them.\\nples are often used to storing data that needs to be accessed in a specific order, such as a list.\\ns are often flexible and can be any type of data.\\n'\n",
      "student argmax tensor([  396, 29896, 29871,    13,   353,    13,   338,   278,  2533,  1546,\n",
      "          278,   322,   322,   263, 29973,  5132, 29973,    13,    13,    13,\n",
      "           13,  3414, 29901,    13, 29874, 29876, 29905, 29915,   338,   263,\n",
      "         1051,   263,   310,   310, 29889,    13,   263,    13,   338,   263,\n",
      "           13,  9246,    13,   310,    13,    13,  1051,   338,    13,    13,\n",
      "        29889,    13,   263,  1051,  2701, 29889, 29889, 29871,  1051,   338,\n",
      "          367,   367,    13,   304,    13,   263, 29889, 29871,   263, 29871,\n",
      "           13, 29889,   278, 29889,    13,  2701, 29889,    13,  1304,   304,\n",
      "        29889, 29889, 29889,   338,   304,   367, 29889, 29889,   263, 29889,\n",
      "         1797, 29889,    13,   408,   263,  1051, 29889,    13, 29879, 29889,\n",
      "           13,    13, 29889, 29871,   367, 29889,  1134,   310,   848, 29889,\n",
      "           13,     1,  1509,    13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  1051,   338,   263, 26691,  5665,   310,  3618, 29892,\n",
      "         1550,   263, 18761,   338,   385,  5198,  9246,  5665,   310,    13,\n",
      "           13,  1051,   508,   367,  3939, 29892,  1550,   263, 18761,  2701,\n",
      "         2609, 29889, 29871,  1051,   338,   367,   367,  1304,   304,  3787,\n",
      "         1819, 29892,  1550,   304,  2189,  6931,   373,   963, 29889,    13,\n",
      "         2701,   526,  4049,  1304,   304, 15446,   848,   393,  4225,   304,\n",
      "          367, 20592,   297,   263,  2702,  1797, 29892,  1316,   408,   263,\n",
      "         1051, 29889,    13, 29879,   526,  4049, 25706,   322,   508,   367,\n",
      "          738,  1134,   310,   848, 29889,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1724,   338,   278,  4328,\n",
      "          1546,   263,  1051,   322, 18761,   297,  5132, 29973, 29871,    13,\n",
      "            13,  3575,   673, 29901,  1839, 29905, 29876, 29909,  1051,   338,\n",
      "           263, 26691,  5665,   310,  3618, 29892,  1550,   263, 18761,   338,\n",
      "           385,  5198,  9246,  5665, 29889, 29871,   319,  1051,   508,   367,\n",
      "          3939, 29892,   541,   263,  5291,  2701,  2609, 29889,   319, 18761,\n",
      "           508,   871,   367,  1304,   304,  3787,  1819, 29892,   451,   304,\n",
      "          2189,  6931,   373,   963, 29889, 12603,  2701,   526,  4049,  1304,\n",
      "           363, 15446,   848,   393,  4225,   304,   367, 20592,   297,   263,\n",
      "          2702,  1797, 29892,  1316,   408,   263,  8600, 29889,  2391, 29879,\n",
      "           526,   901, 25706,   322,   508,  3787,   738,  1134,   310,   848,\n",
      "         29889,     2,   525, 29962]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: What is the difference between a list and tuple in Python? \\n\\n Your Answer:['\\\\nA list is a mutable sequence of objects, while a tuple is an immutable sequence.  A list can be changed, but a tuples cannot. A tuple can only be used to store values, not to perform operations on them. Tuples are often used for storing data that needs to be accessed in a specific order, such as a dictionary. Lists are more flexible and can store any type of data.</s> ']\"\n",
      "input_ids torch.Size([1, 114])\n",
      "attention mask torch.Size([1, 114])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "teacher_logits torch.Size([1, 73, 32001])\n",
      "student_outputs torch.Size([1, 98, 32001])\n",
      "student decoded '#1\\n\\n =\\n to the collection work in Java?\\n\\n\\n\\n task:\\n\\nn\\\\\\n  bage collection. the . Python the\\n..\\nys memory\\n\\n\\n longer\\n\\n.\\n\\n.\\n\\n\\n collectionor. a for the memory memory memory.\\n\\n and laimcling memoryused memory.  is is called..\\n\\n\\n that the Python is to  about.\\n3n <s>use\\n'\n",
      "teacher decoded '\\n\\n\\n. Garbage collection is a process in Python the Python runtime automatically reclaims memory that is no longer in use.\\n\\n. The Python runtime collector is responsible for freeing up memory that identifying and reclaimcling unused objects.  process is done automatically and is not something that the programmer needs to worry about. \\n\\n'\n",
      "student argmax tensor([  396, 29896,    13,    13,   353,    13,   304,   278,  4333,   664,\n",
      "          297,  3355, 29973,    13,    13,    13,    13,  3414, 29901,    13,\n",
      "           13, 29876, 29905,    13, 29871, 29871, 17807,  4333, 29889,   278,\n",
      "        29871, 29889,  5132,   278,    13, 29889, 29889,    13, 29891, 29879,\n",
      "         3370,    13,    13,    13,  5520,    13,    13, 29889,    13,    13,\n",
      "        29889,    13,    13,    13,  4333,   272, 29889,   263,   363,   278,\n",
      "         3370,  3370,  3370, 29889,    13,    13,   322, 29871,  8342, 19914,\n",
      "         3370,  3880,  3370, 29889, 29871,   338,   338,  2000, 29889, 29889,\n",
      "           13,    13,    13,   393,   278,  5132,   338,   304, 29871,  1048,\n",
      "        29889,    13, 29941, 29876, 29871,     1,  1509,    13],\n",
      "       device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  7455, 17807,  4333,   338,   263,  1889,\n",
      "          297,  5132,   278,  5132, 10073,  6336,  1162,  8342, 29879,  3370,\n",
      "          393,   338,   694,  5520,   297,   671, 29889,    13,    13, 29889,\n",
      "          450,  5132, 10073,  6314,   272,   338, 14040,   363,  3889,   292,\n",
      "          701,  3370,   393,  2893,  9215,   322,  1162,  8342, 19914,   443,\n",
      "         3880,  3618, 29889, 29871,  1889,   338,  2309,  6336,   322,   338,\n",
      "          451,  1554,   393,   278, 27922,  4225,   304, 15982,  1048, 29889,\n",
      "        29871,    13,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1128,   947, 25861,  4333,\n",
      "           664,   297,  5132, 29973, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29871, 29896, 29889,  7455, 17807,  4333,   338,\n",
      "           263,  1889,   297,   607,   278,  5132, 10073,  6336,  1162,  8342,\n",
      "         29879,  3370,   393,   338,   694,  5520,   297,   671, 29889, 29871,\n",
      "         29906, 29889,   450,  5132, 25861,  6314,   272,   338, 14040,   363,\n",
      "          3889,   292,   701,  3370,   491,  2893,  9215,   322,  1162, 29891,\n",
      "         19914,   443,  3880,  3618, 29889,   910,  1889,   338,  2309,  6336,\n",
      "           322,   338,   451,  1554,   393,   278, 27922,  4225,   304, 15982,\n",
      "          1048, 29889, 29871,   320, 29876,     2,   525, 29962]],\n",
      "       device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: How does garbage collection work in Python? \\n\\n Your Answer:['\\\\n 1. Garbage collection is a process in which the Python runtime automatically reclaims memory that is no longer in use. 2. The Python garbage collector is responsible for freeing up memory by identifying and recycling unused objects. This process is done automatically and is not something that the programmer needs to worry about.  \\\\n</s> ']\"\n",
      "input_ids torch.Size([1, 98])\n",
      "attention mask torch.Size([1, 98])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 153, 32001])\n",
      "student decoded '#1 \\n =\\nlain the difference of aator in Python.\\n\\n\\n\\n decor:\\n\\nn\\n\\n. orator  .\\n\\n to decor the\\n of a\\n.\\n\\n\\n used to Python..\\n\\n to the\\n.\\n to the the.\\n Python.\\nators\\n.\\n. Python\\n of the decor.\\n.\\n\\nator function. the. the argument. the decorator function.\\n is the decor to be decor. the the..\\n example,\\n decor decor. the to decor the decorator..\\n\\n.\\n decorator functionfunction decor 3.  .name__   decorated 4. defdecorate 5.\\n\\n'\n",
      "teacher decoded \"\\n\\n\\n. Decorators are a that are used to modify the behavior of other functions.\\n are used used to Python languages to add functionality to existing functions. having to rewrite them.\\n Python, decorators functions are used in the form of @function_name.\\n decorator function is passed as an argument to the decorator function.\\n allows the decor to be modified without changing its signature.\\n example, the following code shows how to use a decorator function to\\n\\n. In decorator(func): 3. def func.__name__ + ' decorated 4. @decorate 5.\"\n",
      "student argmax tensor([  396, 29896, 29871,    13,   353,    13,  7420,   278,  4328,   310,\n",
      "          263,  1061,   297,  5132, 29889,    13,    13,    13,    13, 10200,\n",
      "        29901,    13,    13, 29876,    13,    13, 29889, 29871,   272,  1061,\n",
      "        29871, 29871, 29889,    13,    13,   304, 10200,   278,    13,   310,\n",
      "          263,    13, 29889,    13,    13,    13,  1304,   304,  5132, 29889,\n",
      "        29889,    13,    13,   304,   278,    13, 29889,    13,   304,   278,\n",
      "          278, 29889,    13,  5132, 29889,    13,  4097,    13, 29889,    13,\n",
      "        29889,  5132,    13,   310,   278, 10200, 29889,    13, 29889,    13,\n",
      "           13,  1061,   740, 29889,   278, 29889,   278,  2980, 29889,   278,\n",
      "        10200,  1061,   740, 29889,    13,   338,   278, 10200,   304,   367,\n",
      "        10200, 29889,   278,   278, 29889, 29889,    13,  1342, 29892,    13,\n",
      "        10200, 10200, 29889,   278,   304, 10200,   278, 10200,  1061, 29889,\n",
      "        29889,    13,    13, 29889,    13, 10200,  1061,   740,  2220, 10200,\n",
      "        29871, 29941, 29889, 29871, 29871, 29889,   978,  1649, 29871, 29871,\n",
      "        10200,   630, 29871, 29946, 29889,   822, 19557,   403, 29871, 29945,\n",
      "        29889,    13,    13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  3826,   272,  4097,   526,   263,   393,\n",
      "          526,  1304,   304,  6623,   278,  6030,   310,   916,  3168, 29889,\n",
      "           13,   526,  1304,  1304,   304,  5132, 10276,   304,   788,  9863,\n",
      "          304,  5923,  3168, 29889,  2534,   304, 10683,   963, 29889,    13,\n",
      "         5132, 29892, 10200,  4097,  3168,   526,  1304,   297,   278,   883,\n",
      "          310,   732,  2220, 29918,   978, 29889,    13, 10200,  1061,   740,\n",
      "          338,  4502,   408,   385,  2980,   304,   278, 10200,  1061,   740,\n",
      "        29889,    13,  6511,   278, 10200,   304,   367,  9120,  1728,  6480,\n",
      "          967, 12608, 29889,    13,  1342, 29892,   278,  1494,   775,  3697,\n",
      "          920,   304,   671,   263, 10200,  1061,   740,   304,    13,    13,\n",
      "        29889,   512, 10200,  1061, 29898,  9891,  1125, 29871, 29941, 29889,\n",
      "          822,  3653, 17255,   978,  1649,   718,   525, 10200,   630, 29871,\n",
      "        29946, 29889,   732, 19557,   403, 29871, 29945, 29889],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   278,  6964,\n",
      "           310, 10200,  4097,   297,  5132, 29889, 29871,    13,    13,  3575,\n",
      "           673, 29901,  3366, 29905, 29876, 29871, 29896, 29889,  3826,   272,\n",
      "          4097,   526,  3168,   393,   526,  1304,   304,  6623,   278,  6030,\n",
      "           310,   916,  3168, 29889,  2688,   526,  4049,  1304,   297,  8720,\n",
      "         10276,   304,   788,  9863,   304,  5923,  3168,  1728,  2534,   304,\n",
      "         10683,   963, 29889,   512,  5132, 29892, 10200,  1061,  3168,   526,\n",
      "          3971,   297,   278,   883,   310,   732,  2220, 29918,   978, 29889,\n",
      "           450, 10200,   630,   740,   338,  4502,   408,   385,  2980,   304,\n",
      "           278, 10200,  1218,   740, 29889,   910,  6511,   278,   740,   304,\n",
      "           367,  9120,  1728,  6480,   967, 12608, 29889,  1152,  1342, 29892,\n",
      "           278,  1494,   775,  3697,   920,   304,   671,   263, 10200,   362,\n",
      "           740, 29901, 29871, 29906, 29889,   822, 10200,   403, 29898,  9891,\n",
      "          1125, 29871, 29941, 29889,   736,  3653, 17255,   978,  1649,   718,\n",
      "           525, 10200, 29915, 29871, 29946, 29889,   732, 19557,   403, 29871,\n",
      "         29945,  1213, 29962]], device='cuda:0')\n",
      "input_ids decoded '<s> \\n\\nTask: Explain the concept of decorators in Python. \\n\\n Your Answer:[\"\\\\n 1. Decorators are functions that are used to modify the behavior of other functions. They are often used in programming languages to add functionality to existing functions without having to rewrite them. In Python, decorator functions are written in the form of @function_name. The decorated function is passed as an argument to the decorating function. This allows the function to be modified without changing its signature. For example, the following code shows how to use a decoration function: 2. def decorate(func): 3. return func.__name__ + \\' decor\\' 4. @decorate 5.\"]'\n",
      "input_ids torch.Size([1, 153])\n",
      "attention mask torch.Size([1, 153])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Epoch 10, Average Loss: 357.72505950927734\n",
      "Epoch 20, Average Loss: 172.625244140625\n",
      "Epoch 30, Average Loss: 76.4577054977417\n",
      "Epoch 40, Average Loss: 41.8201961517334\n",
      "Epoch 50, Average Loss: 24.112056732177734\n",
      "Epoch 60, Average Loss: 19.062698125839233\n",
      "Epoch 70, Average Loss: 15.124903440475464\n",
      "Epoch 80, Average Loss: 12.906806468963623\n",
      "Epoch 90, Average Loss: 10.970412135124207\n",
      "Epoch 100, Average Loss: 9.926285982131958\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 151, 32001])\n",
      "student decoded '#1\\n\\n \\nlain the to how a search search search\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n binary search is a tree data structure in which each node has at most two children.\\n binary searchsearch algorithm is an algorithm algorithm for searching the item in a sorted array.\\n works by repeatedly dividing the array in half until the desired item is found.\\n algorithm is be used to search for an item in a array, a sorted, or a set.\\n search is a used in computer science to find an element match in a data.\\n example, it can be the index of a given element in the position of a element in a sorted list. Binary addition, binary search can be be used to other problems of'\n",
      "teacher decoded '\\n\\n binary search is a tree data structure in which each node has at most two children.\\n binary searchsearch algorithm is an efficient algorithm for finding an item in a sorted array.\\n works by repeatedly dividing the array in half until the desired item is found.\\n algorithm is be used to search for an item in a array, a sorted, or a tree.\\n search is a used in computer science to find an element match in a data.\\n example, it can be the index of a given element in the position of a element in a sorted array.\\n addition, binary search can be be used to other types of'\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,  7420,   278,   304,   920,\n",
      "          263,  2740,  2740,  2740,    13,    13,    13,    13,    13,    13,\n",
      "           13,    13, 29871,    13,    13,  7581,  2740,   338,   263,  5447,\n",
      "          848,  3829,   297,   607,  1269,  2943,   756,   472,  1556,  1023,\n",
      "         4344, 29889,    13,  7581,  2740,  4478,  5687,   338,   385,  5687,\n",
      "         5687,   363, 11975,   278,  2944,   297,   263, 12705,  1409, 29889,\n",
      "           13,  1736,   491, 28424,  1933,  4821,   278,  1409,   297,  4203,\n",
      "         2745,   278,  7429,  2944,   338,  1476, 29889,    13,  5687,   338,\n",
      "          367,  1304,   304,  2740,   363,   385,  2944,   297,   263,  1409,\n",
      "        29892,   263, 12705, 29892,   470,   263,   731, 29889,    13,  2740,\n",
      "          338,   263,  1304,   297,  6601, 10466,   304,  1284,   385,  1543,\n",
      "         1993,   297,   263,   848, 29889,    13,  1342, 29892,   372,   508,\n",
      "          367,   278,  2380,   310,   263,  2183,  1543,   297,   278,  2602,\n",
      "          310,   263,  1543,   297,   263, 12705,  1051, 29889, 29479,  6124,\n",
      "        29892,  7581,  2740,   508,   367,   367,  1304,   304,   916,  4828,\n",
      "          310], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  7581,  2740,   338,   263,  5447,   848,  3829,   297,\n",
      "          607,  1269,  2943,   756,   472,  1556,  1023,  4344, 29889,    13,\n",
      "         7581,  2740,  4478,  5687,   338,   385,  8543,  5687,   363,  9138,\n",
      "          385,  2944,   297,   263, 12705,  1409, 29889,    13,  1736,   491,\n",
      "        28424,  1933,  4821,   278,  1409,   297,  4203,  2745,   278,  7429,\n",
      "         2944,   338,  1476, 29889,    13,  5687,   338,   367,  1304,   304,\n",
      "         2740,   363,   385,  2944,   297,   263,  1409, 29892,   263, 12705,\n",
      "        29892,   470,   263,  5447, 29889,    13,  2740,   338,   263,  1304,\n",
      "          297,  6601, 10466,   304,  1284,   385,  1543,  1993,   297,   263,\n",
      "          848, 29889,    13,  1342, 29892,   372,   508,   367,   278,  2380,\n",
      "          310,   263,  2183,  1543,   297,   278,  2602,   310,   263,  1543,\n",
      "          297,   263, 12705,  1409, 29889,    13,  6124, 29892,  7581,  2740,\n",
      "          508,   367,   367,  1304,   304,   916,  4072,   310],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   920,   263,\n",
      "          7581,  2740,  1736, 29889, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29909,  7581,  5447,   338,   263,  5447,   848,\n",
      "          3829,   297,   607,  1269,  2943,   756,   472,  1556,  1023,  4344,\n",
      "         29889,   450,  7581, 29899,  4478,  5687,   338,   385,  8543,  5687,\n",
      "           363,  9138,   385,  2944,   297,   263, 12705,  1409, 29889,   739,\n",
      "          1736,   491, 28424,  1933,  4821,   278,  1409,   297,  4203,  2745,\n",
      "           278,  7429,  2944,   338,  1476, 29889,   910,  5687,   508,   367,\n",
      "          1304,   304,  2740,   363,   385,  1543,   297,   385,  1409, 29892,\n",
      "           263,  1051, 29892,   470,   263,   731, 29889, 29479,  2740,   338,\n",
      "          4049,  1304,   297,  6601, 10466,   304,  1284,   385,  2684,  1993,\n",
      "           297, 12705,   848, 29889,  1152,  1342, 29892,   372,   508,  1284,\n",
      "           278,  2380,   310,   263,  2183,  1543,   470,   278,  2602,   310,\n",
      "           385,  1203,   297,   278, 12705,  1051, 29889,   512,  6124, 29892,\n",
      "          7581, 29645,   508,   884,   367,  7436,   304,   916,  4072,   310,\n",
      "          2033]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: Explain how a binary search works. \\n\\n Your Answer:['\\\\nA binary tree is a tree data structure in which each node has at most two children. The binary-search algorithm is an efficient algorithm for finding an item in a sorted array. It works by repeatedly dividing the array in half until the desired item is found. This algorithm can be used to search for an element in an array, a list, or a set. Binary search is often used in computer science to find an exact match in sorted data. For example, it can find the index of a given element or the position of an object in the sorted list. In addition, binary searches can also be applied to other types of']\"\n",
      "input_ids torch.Size([1, 151])\n",
      "attention mask torch.Size([1, 151])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 86, 32001])\n",
      "student_outputs torch.Size([1, 114, 32001])\n",
      "student decoded '#1\\n\\n \\n is\\n task between the and and a and and\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n list is a mutable sequence of objects, while a tuple is an immutable sequence of\\n\\n list can be changed, while a tupleples cannot.  list is be be used to store data, while to perform operations. them.\\nples are often used to storing data that does to be accessed in a specific order, such as a list.\\ns are often flexible and can be any type of data,\\n'\n",
      "teacher decoded '\\n\\n list is a mutable sequence of objects, while a tuple is an immutable sequence of\\n\\n list can be changed, while a tupleples cannot.  list is be be used to store values, while to perform operations on them.\\nples are often used to storing data that needs to be accessed in a specific order, such as a list.\\ns are often flexible and can be any type of data.\\n'\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,   338,    13,  3414,  1546,\n",
      "          278,   322,   322,   263,   322,   322,    13,    13,    13,    13,\n",
      "           13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
      "         1051,   338,   263, 26691,  5665,   310,  3618, 29892,  1550,   263,\n",
      "        18761,   338,   385,  5198,  9246,  5665,   310,    13,    13,  1051,\n",
      "          508,   367,  3939, 29892,  1550,   263, 18761,  2701,  2609, 29889,\n",
      "        29871,  1051,   338,   367,   367,  1304,   304,  3787,   848, 29892,\n",
      "         1550,   304,  2189,  6931, 29889,   963, 29889,    13,  2701,   526,\n",
      "         4049,  1304,   304, 15446,   848,   393,   947,   304,   367, 20592,\n",
      "          297,   263,  2702,  1797, 29892,  1316,   408,   263,  1051, 29889,\n",
      "           13, 29879,   526,  4049, 25706,   322,   508,   367,   738,  1134,\n",
      "          310,   848, 29892,    13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  1051,   338,   263, 26691,  5665,   310,  3618, 29892,\n",
      "         1550,   263, 18761,   338,   385,  5198,  9246,  5665,   310,    13,\n",
      "           13,  1051,   508,   367,  3939, 29892,  1550,   263, 18761,  2701,\n",
      "         2609, 29889, 29871,  1051,   338,   367,   367,  1304,   304,  3787,\n",
      "         1819, 29892,  1550,   304,  2189,  6931,   373,   963, 29889,    13,\n",
      "         2701,   526,  4049,  1304,   304, 15446,   848,   393,  4225,   304,\n",
      "          367, 20592,   297,   263,  2702,  1797, 29892,  1316,   408,   263,\n",
      "         1051, 29889,    13, 29879,   526,  4049, 25706,   322,   508,   367,\n",
      "          738,  1134,   310,   848, 29889,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1724,   338,   278,  4328,\n",
      "          1546,   263,  1051,   322, 18761,   297,  5132, 29973, 29871,    13,\n",
      "            13,  3575,   673, 29901,  1839, 29905, 29876, 29909,  1051,   338,\n",
      "           263, 26691,  5665,   310,  3618, 29892,  1550,   263, 18761,   338,\n",
      "           385,  5198,  9246,  5665, 29889, 29871,   319,  1051,   508,   367,\n",
      "          3939, 29892,   541,   263,  5291,  2701,  2609, 29889,   319, 18761,\n",
      "           508,   871,   367,  1304,   304,  3787,  1819, 29892,   451,   304,\n",
      "          2189,  6931,   373,   963, 29889, 12603,  2701,   526,  4049,  1304,\n",
      "           363, 15446,   848,   393,  4225,   304,   367, 20592,   297,   263,\n",
      "          2702,  1797, 29892,  1316,   408,   263,  8600, 29889,  2391, 29879,\n",
      "           526,   901, 25706,   322,   508,  3787,   738,  1134,   310,   848,\n",
      "         29889,     2,   525, 29962]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: What is the difference between a list and tuple in Python? \\n\\n Your Answer:['\\\\nA list is a mutable sequence of objects, while a tuple is an immutable sequence.  A list can be changed, but a tuples cannot. A tuple can only be used to store values, not to perform operations on them. Tuples are often used for storing data that needs to be accessed in a specific order, such as a dictionary. Lists are more flexible and can store any type of data.</s> ']\"\n",
      "input_ids torch.Size([1, 114])\n",
      "attention mask torch.Size([1, 114])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "teacher_logits torch.Size([1, 73, 32001])\n",
      "student_outputs torch.Size([1, 98, 32001])\n",
      "student decoded '#1\\n\\n \\n to the collect works garbage work\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n. Pythonbage collection is a process in Python Python Python runtime automatically reclaims memory that is no longer in use by\\n\\n. The Python runtime collector is a for freeing up memory that identifying and reclaimcling unused objects.  helps is done automatically and is usually something that the user needs to worry about. 3\\n'\n",
      "teacher decoded '\\n\\n\\n. Garbage collection is a process in Python the Python runtime automatically reclaims memory that is no longer in use.\\n\\n. The Python runtime collector is responsible for freeing up memory that identifying and reclaimcling unused objects.  process is done automatically and is not something that the programmer needs to worry about. \\n\\n'\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,   304,   278,  6314,  1736,\n",
      "        25861,   664,    13,    13,    13,    13,    13,    13,    13,    13,\n",
      "           13,    13,    13,    13,    13,    13,    13,    13, 29889,  5132,\n",
      "        17807,  4333,   338,   263,  1889,   297,  5132,  5132,  5132, 10073,\n",
      "         6336,  1162,  8342, 29879,  3370,   393,   338,   694,  5520,   297,\n",
      "          671,   491,    13,    13, 29889,   450,  5132, 10073,  6314,   272,\n",
      "          338,   263,   363,  3889,   292,   701,  3370,   393,  2893,  9215,\n",
      "          322,  1162,  8342, 19914,   443,  3880,  3618, 29889, 29871,  6911,\n",
      "          338,  2309,  6336,   322,   338,  5491,  1554,   393,   278,  1404,\n",
      "         4225,   304, 15982,  1048, 29889, 29871, 29941,    13],\n",
      "       device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  7455, 17807,  4333,   338,   263,  1889,\n",
      "          297,  5132,   278,  5132, 10073,  6336,  1162,  8342, 29879,  3370,\n",
      "          393,   338,   694,  5520,   297,   671, 29889,    13,    13, 29889,\n",
      "          450,  5132, 10073,  6314,   272,   338, 14040,   363,  3889,   292,\n",
      "          701,  3370,   393,  2893,  9215,   322,  1162,  8342, 19914,   443,\n",
      "         3880,  3618, 29889, 29871,  1889,   338,  2309,  6336,   322,   338,\n",
      "          451,  1554,   393,   278, 27922,  4225,   304, 15982,  1048, 29889,\n",
      "        29871,    13,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1128,   947, 25861,  4333,\n",
      "           664,   297,  5132, 29973, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29871, 29896, 29889,  7455, 17807,  4333,   338,\n",
      "           263,  1889,   297,   607,   278,  5132, 10073,  6336,  1162,  8342,\n",
      "         29879,  3370,   393,   338,   694,  5520,   297,   671, 29889, 29871,\n",
      "         29906, 29889,   450,  5132, 25861,  6314,   272,   338, 14040,   363,\n",
      "          3889,   292,   701,  3370,   491,  2893,  9215,   322,  1162, 29891,\n",
      "         19914,   443,  3880,  3618, 29889,   910,  1889,   338,  2309,  6336,\n",
      "           322,   338,   451,  1554,   393,   278, 27922,  4225,   304, 15982,\n",
      "          1048, 29889, 29871,   320, 29876,     2,   525, 29962]],\n",
      "       device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: How does garbage collection work in Python? \\n\\n Your Answer:['\\\\n 1. Garbage collection is a process in which the Python runtime automatically reclaims memory that is no longer in use. 2. The Python garbage collector is responsible for freeing up memory by identifying and recycling unused objects. This process is done automatically and is not something that the programmer needs to worry about.  \\\\n</s> ']\"\n",
      "input_ids torch.Size([1, 98])\n",
      "attention mask torch.Size([1, 98])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 153, 32001])\n",
      "student decoded \"#1\\n\\n \\nlain the the of theator and decor Python Python\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n. Decorators are a that are used to modify the behavior of another functions.\\n are often used to Python languages to add additional to existing functions. having to rewrite them.\\n Python, decorators functions are used in a form of afunction_name.\\n functionator function is called as an argument to the decorator function.\\n allows the decor to be called without having the name.\\n example, the following code shows how to use a decorator to to\\n\\n. In addator(func): 3. def func(name__ + '()ated 4. @decorate 5.\"\n",
      "teacher decoded \"\\n\\n\\n. Decorators are a that are used to modify the behavior of other functions.\\n are used used to Python languages to add functionality to existing functions. having to rewrite them.\\n Python, decorators functions are used in the form of @function_name.\\n decorator function is passed as an argument to the decorator function.\\n allows the decor to be modified without changing its signature.\\n example, the following code shows how to use a decorator function to\\n\\n. In decorator(func): 3. def func.__name__ + ' decorated 4. @decorate 5.\"\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,  7420,   278,   278,   310,\n",
      "          278,  1061,   322, 10200,  5132,  5132,    13,    13,    13,    13,\n",
      "           13,    13,    13,    13,    13,    13,    13,    13, 29889,  3826,\n",
      "          272,  4097,   526,   263,   393,   526,  1304,   304,  6623,   278,\n",
      "         6030,   310,  1790,  3168, 29889,    13,   526,  4049,  1304,   304,\n",
      "         5132, 10276,   304,   788,  5684,   304,  5923,  3168, 29889,  2534,\n",
      "          304, 10683,   963, 29889,    13,  5132, 29892, 10200,  4097,  3168,\n",
      "          526,  1304,   297,   263,   883,   310,   263,  2220, 29918,   978,\n",
      "        29889,    13,   740,  1061,   740,   338,  2000,   408,   385,  2980,\n",
      "          304,   278, 10200,  1061,   740, 29889,    13,  6511,   278, 10200,\n",
      "          304,   367,  2000,  1728,  2534,   278,  1024, 29889,    13,  1342,\n",
      "        29892,   278,  1494,   775,  3697,   920,   304,   671,   263, 10200,\n",
      "         1061,   304,   304,    13,    13, 29889,   512,   788,  1061, 29898,\n",
      "         9891,  1125, 29871, 29941, 29889,   822,  3653, 29898,   978,  1649,\n",
      "          718,   525,   580,   630, 29871, 29946, 29889,   732, 19557,   403,\n",
      "        29871, 29945, 29889], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  3826,   272,  4097,   526,   263,   393,\n",
      "          526,  1304,   304,  6623,   278,  6030,   310,   916,  3168, 29889,\n",
      "           13,   526,  1304,  1304,   304,  5132, 10276,   304,   788,  9863,\n",
      "          304,  5923,  3168, 29889,  2534,   304, 10683,   963, 29889,    13,\n",
      "         5132, 29892, 10200,  4097,  3168,   526,  1304,   297,   278,   883,\n",
      "          310,   732,  2220, 29918,   978, 29889,    13, 10200,  1061,   740,\n",
      "          338,  4502,   408,   385,  2980,   304,   278, 10200,  1061,   740,\n",
      "        29889,    13,  6511,   278, 10200,   304,   367,  9120,  1728,  6480,\n",
      "          967, 12608, 29889,    13,  1342, 29892,   278,  1494,   775,  3697,\n",
      "          920,   304,   671,   263, 10200,  1061,   740,   304,    13,    13,\n",
      "        29889,   512, 10200,  1061, 29898,  9891,  1125, 29871, 29941, 29889,\n",
      "          822,  3653, 17255,   978,  1649,   718,   525, 10200,   630, 29871,\n",
      "        29946, 29889,   732, 19557,   403, 29871, 29945, 29889],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   278,  6964,\n",
      "           310, 10200,  4097,   297,  5132, 29889, 29871,    13,    13,  3575,\n",
      "           673, 29901,  3366, 29905, 29876, 29871, 29896, 29889,  3826,   272,\n",
      "          4097,   526,  3168,   393,   526,  1304,   304,  6623,   278,  6030,\n",
      "           310,   916,  3168, 29889,  2688,   526,  4049,  1304,   297,  8720,\n",
      "         10276,   304,   788,  9863,   304,  5923,  3168,  1728,  2534,   304,\n",
      "         10683,   963, 29889,   512,  5132, 29892, 10200,  1061,  3168,   526,\n",
      "          3971,   297,   278,   883,   310,   732,  2220, 29918,   978, 29889,\n",
      "           450, 10200,   630,   740,   338,  4502,   408,   385,  2980,   304,\n",
      "           278, 10200,  1218,   740, 29889,   910,  6511,   278,   740,   304,\n",
      "           367,  9120,  1728,  6480,   967, 12608, 29889,  1152,  1342, 29892,\n",
      "           278,  1494,   775,  3697,   920,   304,   671,   263, 10200,   362,\n",
      "           740, 29901, 29871, 29906, 29889,   822, 10200,   403, 29898,  9891,\n",
      "          1125, 29871, 29941, 29889,   736,  3653, 17255,   978,  1649,   718,\n",
      "           525, 10200, 29915, 29871, 29946, 29889,   732, 19557,   403, 29871,\n",
      "         29945,  1213, 29962]], device='cuda:0')\n",
      "input_ids decoded '<s> \\n\\nTask: Explain the concept of decorators in Python. \\n\\n Your Answer:[\"\\\\n 1. Decorators are functions that are used to modify the behavior of other functions. They are often used in programming languages to add functionality to existing functions without having to rewrite them. In Python, decorator functions are written in the form of @function_name. The decorated function is passed as an argument to the decorating function. This allows the function to be modified without changing its signature. For example, the following code shows how to use a decoration function: 2. def decorate(func): 3. return func.__name__ + \\' decor\\' 4. @decorate 5.\"]'\n",
      "input_ids torch.Size([1, 153])\n",
      "attention mask torch.Size([1, 153])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Epoch 110, Average Loss: 9.937899470329285\n",
      "Epoch 120, Average Loss: 7.64973521232605\n",
      "Epoch 130, Average Loss: 7.420369386672974\n",
      "Epoch 140, Average Loss: 6.823510527610779\n",
      "Epoch 150, Average Loss: 6.85212254524231\n",
      "Epoch 160, Average Loss: 6.323798060417175\n",
      "Epoch 170, Average Loss: 5.471654891967773\n",
      "Epoch 180, Average Loss: 5.432908892631531\n",
      "Epoch 190, Average Loss: 5.083137273788452\n",
      "Epoch 200, Average Loss: 4.640601754188538\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 151, 32001])\n",
      "student decoded '#1\\n\\n \\nlain the to  a search search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n binary search is a tree data structure in which each node has at most two children.\\n binary searchsearch algorithm is an algorithm algorithm for finding an item in a sorted array.\\n works by repeatedly dividing the array in half until the desired item is found.\\n algorithm is be used to find for an item in a array, a binary, or a binary.\\n search is a used in computer science to find an element match in a data.\\n example, it can be the index of a value element in the position of a element in a list list.\\n addition, it search can be be used to other types of'\n",
      "teacher decoded '\\n\\n binary search is a tree data structure in which each node has at most two children.\\n binary searchsearch algorithm is an efficient algorithm for finding an item in a sorted array.\\n works by repeatedly dividing the array in half until the desired item is found.\\n algorithm is be used to search for an item in a array, a sorted, or a tree.\\n search is a used in computer science to find an element match in a data.\\n example, it can be the index of a given element in the position of a element in a sorted array.\\n addition, binary search can be be used to other types of'\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,  7420,   278,   304, 29871,\n",
      "          263,  2740,  2740,    13,    13,    13,    13,    13,    13,    13,\n",
      "           13,    13,    13,    13,    13,  7581,  2740,   338,   263,  5447,\n",
      "          848,  3829,   297,   607,  1269,  2943,   756,   472,  1556,  1023,\n",
      "         4344, 29889,    13,  7581,  2740,  4478,  5687,   338,   385,  5687,\n",
      "         5687,   363,  9138,   385,  2944,   297,   263, 12705,  1409, 29889,\n",
      "           13,  1736,   491, 28424,  1933,  4821,   278,  1409,   297,  4203,\n",
      "         2745,   278,  7429,  2944,   338,  1476, 29889,    13,  5687,   338,\n",
      "          367,  1304,   304,  1284,   363,   385,  2944,   297,   263,  1409,\n",
      "        29892,   263,  7581, 29892,   470,   263,  7581, 29889,    13,  2740,\n",
      "          338,   263,  1304,   297,  6601, 10466,   304,  1284,   385,  1543,\n",
      "         1993,   297,   263,   848, 29889,    13,  1342, 29892,   372,   508,\n",
      "          367,   278,  2380,   310,   263,   995,  1543,   297,   278,  2602,\n",
      "          310,   263,  1543,   297,   263,  1051,  1051, 29889,    13,  6124,\n",
      "        29892,   372,  2740,   508,   367,   367,  1304,   304,   916,  4072,\n",
      "          310], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  7581,  2740,   338,   263,  5447,   848,  3829,   297,\n",
      "          607,  1269,  2943,   756,   472,  1556,  1023,  4344, 29889,    13,\n",
      "         7581,  2740,  4478,  5687,   338,   385,  8543,  5687,   363,  9138,\n",
      "          385,  2944,   297,   263, 12705,  1409, 29889,    13,  1736,   491,\n",
      "        28424,  1933,  4821,   278,  1409,   297,  4203,  2745,   278,  7429,\n",
      "         2944,   338,  1476, 29889,    13,  5687,   338,   367,  1304,   304,\n",
      "         2740,   363,   385,  2944,   297,   263,  1409, 29892,   263, 12705,\n",
      "        29892,   470,   263,  5447, 29889,    13,  2740,   338,   263,  1304,\n",
      "          297,  6601, 10466,   304,  1284,   385,  1543,  1993,   297,   263,\n",
      "          848, 29889,    13,  1342, 29892,   372,   508,   367,   278,  2380,\n",
      "          310,   263,  2183,  1543,   297,   278,  2602,   310,   263,  1543,\n",
      "          297,   263, 12705,  1409, 29889,    13,  6124, 29892,  7581,  2740,\n",
      "          508,   367,   367,  1304,   304,   916,  4072,   310],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   920,   263,\n",
      "          7581,  2740,  1736, 29889, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29909,  7581,  5447,   338,   263,  5447,   848,\n",
      "          3829,   297,   607,  1269,  2943,   756,   472,  1556,  1023,  4344,\n",
      "         29889,   450,  7581, 29899,  4478,  5687,   338,   385,  8543,  5687,\n",
      "           363,  9138,   385,  2944,   297,   263, 12705,  1409, 29889,   739,\n",
      "          1736,   491, 28424,  1933,  4821,   278,  1409,   297,  4203,  2745,\n",
      "           278,  7429,  2944,   338,  1476, 29889,   910,  5687,   508,   367,\n",
      "          1304,   304,  2740,   363,   385,  1543,   297,   385,  1409, 29892,\n",
      "           263,  1051, 29892,   470,   263,   731, 29889, 29479,  2740,   338,\n",
      "          4049,  1304,   297,  6601, 10466,   304,  1284,   385,  2684,  1993,\n",
      "           297, 12705,   848, 29889,  1152,  1342, 29892,   372,   508,  1284,\n",
      "           278,  2380,   310,   263,  2183,  1543,   470,   278,  2602,   310,\n",
      "           385,  1203,   297,   278, 12705,  1051, 29889,   512,  6124, 29892,\n",
      "          7581, 29645,   508,   884,   367,  7436,   304,   916,  4072,   310,\n",
      "          2033]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: Explain how a binary search works. \\n\\n Your Answer:['\\\\nA binary tree is a tree data structure in which each node has at most two children. The binary-search algorithm is an efficient algorithm for finding an item in a sorted array. It works by repeatedly dividing the array in half until the desired item is found. This algorithm can be used to search for an element in an array, a list, or a set. Binary search is often used in computer science to find an exact match in sorted data. For example, it can find the index of a given element or the position of an object in the sorted list. In addition, binary searches can also be applied to other types of']\"\n",
      "input_ids torch.Size([1, 151])\n",
      "attention mask torch.Size([1, 151])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 86, 32001])\n",
      "student_outputs torch.Size([1, 114, 32001])\n",
      "student decoded '#1\\n\\n \\n is the task between the and and a and and\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n list is a mutable sequence of objects, while a tuple is an immutable sequence of\\n\\n list can be changed, while a tupleples cannot.  list is be be assigned to store values, while to perform operations on them.\\nples are often used to storing data that needs to be accessed in a specific order. such as a list.\\ns are often flexible and can be any type of data.\\n'\n",
      "teacher decoded '\\n\\n list is a mutable sequence of objects, while a tuple is an immutable sequence of\\n\\n list can be changed, while a tupleples cannot.  list is be be used to store values, while to perform operations on them.\\nples are often used to storing data that needs to be accessed in a specific order, such as a list.\\ns are often flexible and can be any type of data.\\n'\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,   338,   278,  3414,  1546,\n",
      "          278,   322,   322,   263,   322,   322,    13,    13,    13,    13,\n",
      "           13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
      "         1051,   338,   263, 26691,  5665,   310,  3618, 29892,  1550,   263,\n",
      "        18761,   338,   385,  5198,  9246,  5665,   310,    13,    13,  1051,\n",
      "          508,   367,  3939, 29892,  1550,   263, 18761,  2701,  2609, 29889,\n",
      "        29871,  1051,   338,   367,   367,  9859,   304,  3787,  1819, 29892,\n",
      "         1550,   304,  2189,  6931,   373,   963, 29889,    13,  2701,   526,\n",
      "         4049,  1304,   304, 15446,   848,   393,  4225,   304,   367, 20592,\n",
      "          297,   263,  2702,  1797, 29889,  1316,   408,   263,  1051, 29889,\n",
      "           13, 29879,   526,  4049, 25706,   322,   508,   367,   738,  1134,\n",
      "          310,   848, 29889,    13], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,  1051,   338,   263, 26691,  5665,   310,  3618, 29892,\n",
      "         1550,   263, 18761,   338,   385,  5198,  9246,  5665,   310,    13,\n",
      "           13,  1051,   508,   367,  3939, 29892,  1550,   263, 18761,  2701,\n",
      "         2609, 29889, 29871,  1051,   338,   367,   367,  1304,   304,  3787,\n",
      "         1819, 29892,  1550,   304,  2189,  6931,   373,   963, 29889,    13,\n",
      "         2701,   526,  4049,  1304,   304, 15446,   848,   393,  4225,   304,\n",
      "          367, 20592,   297,   263,  2702,  1797, 29892,  1316,   408,   263,\n",
      "         1051, 29889,    13, 29879,   526,  4049, 25706,   322,   508,   367,\n",
      "          738,  1134,   310,   848, 29889,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1724,   338,   278,  4328,\n",
      "          1546,   263,  1051,   322, 18761,   297,  5132, 29973, 29871,    13,\n",
      "            13,  3575,   673, 29901,  1839, 29905, 29876, 29909,  1051,   338,\n",
      "           263, 26691,  5665,   310,  3618, 29892,  1550,   263, 18761,   338,\n",
      "           385,  5198,  9246,  5665, 29889, 29871,   319,  1051,   508,   367,\n",
      "          3939, 29892,   541,   263,  5291,  2701,  2609, 29889,   319, 18761,\n",
      "           508,   871,   367,  1304,   304,  3787,  1819, 29892,   451,   304,\n",
      "          2189,  6931,   373,   963, 29889, 12603,  2701,   526,  4049,  1304,\n",
      "           363, 15446,   848,   393,  4225,   304,   367, 20592,   297,   263,\n",
      "          2702,  1797, 29892,  1316,   408,   263,  8600, 29889,  2391, 29879,\n",
      "           526,   901, 25706,   322,   508,  3787,   738,  1134,   310,   848,\n",
      "         29889,     2,   525, 29962]], device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: What is the difference between a list and tuple in Python? \\n\\n Your Answer:['\\\\nA list is a mutable sequence of objects, while a tuple is an immutable sequence.  A list can be changed, but a tuples cannot. A tuple can only be used to store values, not to perform operations on them. Tuples are often used for storing data that needs to be accessed in a specific order, such as a dictionary. Lists are more flexible and can store any type of data.</s> ']\"\n",
      "input_ids torch.Size([1, 114])\n",
      "attention mask torch.Size([1, 114])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "teacher_logits torch.Size([1, 73, 32001])\n",
      "student_outputs torch.Size([1, 98, 32001])\n",
      "student decoded '#1\\n\\n \\n to the collect garbage how work\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n. Garbage collection is a process in Python the Python runtime automatically reclaims memory that is no longer in use.\\n\\n. The Python runtime collector is responsible for identing up memory that identifying and reclaimcling unused objects.  process is done automatically and is not something that the programmer needs to worry about. 3\\n'\n",
      "teacher decoded '\\n\\n\\n. Garbage collection is a process in Python the Python runtime automatically reclaims memory that is no longer in use.\\n\\n. The Python runtime collector is responsible for freeing up memory that identifying and reclaimcling unused objects.  process is done automatically and is not something that the programmer needs to worry about. \\n\\n'\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,   304,   278,  6314, 25861,\n",
      "          920,   664,    13,    13,    13,    13,    13,    13,    13,    13,\n",
      "           13,    13,    13,    13,    13,    13,    13,    13, 29889,  7455,\n",
      "        17807,  4333,   338,   263,  1889,   297,  5132,   278,  5132, 10073,\n",
      "         6336,  1162,  8342, 29879,  3370,   393,   338,   694,  5520,   297,\n",
      "          671, 29889,    13,    13, 29889,   450,  5132, 10073,  6314,   272,\n",
      "          338, 14040,   363,  2893,   292,   701,  3370,   393,  2893,  9215,\n",
      "          322,  1162,  8342, 19914,   443,  3880,  3618, 29889, 29871,  1889,\n",
      "          338,  2309,  6336,   322,   338,   451,  1554,   393,   278, 27922,\n",
      "         4225,   304, 15982,  1048, 29889, 29871, 29941,    13],\n",
      "       device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  7455, 17807,  4333,   338,   263,  1889,\n",
      "          297,  5132,   278,  5132, 10073,  6336,  1162,  8342, 29879,  3370,\n",
      "          393,   338,   694,  5520,   297,   671, 29889,    13,    13, 29889,\n",
      "          450,  5132, 10073,  6314,   272,   338, 14040,   363,  3889,   292,\n",
      "          701,  3370,   393,  2893,  9215,   322,  1162,  8342, 19914,   443,\n",
      "         3880,  3618, 29889, 29871,  1889,   338,  2309,  6336,   322,   338,\n",
      "          451,  1554,   393,   278, 27922,  4225,   304, 15982,  1048, 29889,\n",
      "        29871,    13,    13], device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901,  1128,   947, 25861,  4333,\n",
      "           664,   297,  5132, 29973, 29871,    13,    13,  3575,   673, 29901,\n",
      "          1839, 29905, 29876, 29871, 29896, 29889,  7455, 17807,  4333,   338,\n",
      "           263,  1889,   297,   607,   278,  5132, 10073,  6336,  1162,  8342,\n",
      "         29879,  3370,   393,   338,   694,  5520,   297,   671, 29889, 29871,\n",
      "         29906, 29889,   450,  5132, 25861,  6314,   272,   338, 14040,   363,\n",
      "          3889,   292,   701,  3370,   491,  2893,  9215,   322,  1162, 29891,\n",
      "         19914,   443,  3880,  3618, 29889,   910,  1889,   338,  2309,  6336,\n",
      "           322,   338,   451,  1554,   393,   278, 27922,  4225,   304, 15982,\n",
      "          1048, 29889, 29871,   320, 29876,     2,   525, 29962]],\n",
      "       device='cuda:0')\n",
      "input_ids decoded \"<s> \\n\\nTask: How does garbage collection work in Python? \\n\\n Your Answer:['\\\\n 1. Garbage collection is a process in which the Python runtime automatically reclaims memory that is no longer in use. 2. The Python garbage collector is responsible for freeing up memory by identifying and recycling unused objects. This process is done automatically and is not something that the programmer needs to worry about.  \\\\n</s> ']\"\n",
      "input_ids torch.Size([1, 98])\n",
      "attention mask torch.Size([1, 98])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "teacher_logits torch.Size([1, 128, 32001])\n",
      "student_outputs torch.Size([1, 153, 32001])\n",
      "student decoded \"#1\\n\\n \\nlain the the of theator and decor in Python\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n. Decorators are a that are used to modify the behavior of other functions.\\n are often used to Python languages to add additional to existing functions. having to rewrite them.\\n Python, decorators functions are used in the form of @function_name.\\n decorator function is passed as an argument to the decorator function.\\n allows the decor to be modified without having its name.\\n example, the following code shows how to use a decorator function to\\n\\n. In decorator_func): 3. def func.__name__ + ' decorated 4. @decorate 5.\"\n",
      "teacher decoded \"\\n\\n\\n. Decorators are a that are used to modify the behavior of other functions.\\n are used used to Python languages to add functionality to existing functions. having to rewrite them.\\n Python, decorators functions are used in the form of @function_name.\\n decorator function is passed as an argument to the decorator function.\\n allows the decor to be modified without changing its signature.\\n example, the following code shows how to use a decorator function to\\n\\n. In decorator(func): 3. def func.__name__ + ' decorated 4. @decorate 5.\"\n",
      "student argmax tensor([  396, 29896,    13,    13, 29871,    13,  7420,   278,   278,   310,\n",
      "          278,  1061,   322, 10200,   297,  5132,    13,    13,    13,    13,\n",
      "           13, 29871,    13,    13,    13,    13,    13,    13, 29889,  3826,\n",
      "          272,  4097,   526,   263,   393,   526,  1304,   304,  6623,   278,\n",
      "         6030,   310,   916,  3168, 29889,    13,   526,  4049,  1304,   304,\n",
      "         5132, 10276,   304,   788,  5684,   304,  5923,  3168, 29889,  2534,\n",
      "          304, 10683,   963, 29889,    13,  5132, 29892, 10200,  4097,  3168,\n",
      "          526,  1304,   297,   278,   883,   310,   732,  2220, 29918,   978,\n",
      "        29889,    13, 10200,  1061,   740,   338,  4502,   408,   385,  2980,\n",
      "          304,   278, 10200,  1061,   740, 29889,    13,  6511,   278, 10200,\n",
      "          304,   367,  9120,  1728,  2534,   967,  1024, 29889,    13,  1342,\n",
      "        29892,   278,  1494,   775,  3697,   920,   304,   671,   263, 10200,\n",
      "         1061,   740,   304,    13,    13, 29889,   512, 10200,  1061, 29918,\n",
      "         9891,  1125, 29871, 29941, 29889,   822,  3653, 17255,   978,  1649,\n",
      "          718,   525, 10200,   630, 29871, 29946, 29889,   732, 19557,   403,\n",
      "        29871, 29945, 29889], device='cuda:0')\n",
      "teacher argmax tensor([   13,    13,    13, 29889,  3826,   272,  4097,   526,   263,   393,\n",
      "          526,  1304,   304,  6623,   278,  6030,   310,   916,  3168, 29889,\n",
      "           13,   526,  1304,  1304,   304,  5132, 10276,   304,   788,  9863,\n",
      "          304,  5923,  3168, 29889,  2534,   304, 10683,   963, 29889,    13,\n",
      "         5132, 29892, 10200,  4097,  3168,   526,  1304,   297,   278,   883,\n",
      "          310,   732,  2220, 29918,   978, 29889,    13, 10200,  1061,   740,\n",
      "          338,  4502,   408,   385,  2980,   304,   278, 10200,  1061,   740,\n",
      "        29889,    13,  6511,   278, 10200,   304,   367,  9120,  1728,  6480,\n",
      "          967, 12608, 29889,    13,  1342, 29892,   278,  1494,   775,  3697,\n",
      "          920,   304,   671,   263, 10200,  1061,   740,   304,    13,    13,\n",
      "        29889,   512, 10200,  1061, 29898,  9891,  1125, 29871, 29941, 29889,\n",
      "          822,  3653, 17255,   978,  1649,   718,   525, 10200,   630, 29871,\n",
      "        29946, 29889,   732, 19557,   403, 29871, 29945, 29889],\n",
      "       device='cuda:0')\n",
      "input_ids tensor([[    1, 29871,    13,    13,  5398, 29901, 12027,  7420,   278,  6964,\n",
      "           310, 10200,  4097,   297,  5132, 29889, 29871,    13,    13,  3575,\n",
      "           673, 29901,  3366, 29905, 29876, 29871, 29896, 29889,  3826,   272,\n",
      "          4097,   526,  3168,   393,   526,  1304,   304,  6623,   278,  6030,\n",
      "           310,   916,  3168, 29889,  2688,   526,  4049,  1304,   297,  8720,\n",
      "         10276,   304,   788,  9863,   304,  5923,  3168,  1728,  2534,   304,\n",
      "         10683,   963, 29889,   512,  5132, 29892, 10200,  1061,  3168,   526,\n",
      "          3971,   297,   278,   883,   310,   732,  2220, 29918,   978, 29889,\n",
      "           450, 10200,   630,   740,   338,  4502,   408,   385,  2980,   304,\n",
      "           278, 10200,  1218,   740, 29889,   910,  6511,   278,   740,   304,\n",
      "           367,  9120,  1728,  6480,   967, 12608, 29889,  1152,  1342, 29892,\n",
      "           278,  1494,   775,  3697,   920,   304,   671,   263, 10200,   362,\n",
      "           740, 29901, 29871, 29906, 29889,   822, 10200,   403, 29898,  9891,\n",
      "          1125, 29871, 29941, 29889,   736,  3653, 17255,   978,  1649,   718,\n",
      "           525, 10200, 29915, 29871, 29946, 29889,   732, 19557,   403, 29871,\n",
      "         29945,  1213, 29962]], device='cuda:0')\n",
      "input_ids decoded '<s> \\n\\nTask: Explain the concept of decorators in Python. \\n\\n Your Answer:[\"\\\\n 1. Decorators are functions that are used to modify the behavior of other functions. They are often used in programming languages to add functionality to existing functions without having to rewrite them. In Python, decorator functions are written in the form of @function_name. The decorated function is passed as an argument to the decorating function. This allows the function to be modified without changing its signature. For example, the following code shows how to use a decoration function: 2. def decorate(func): 3. return func.__name__ + \\' decor\\' 4. @decorate 5.\"]'\n",
      "input_ids torch.Size([1, 153])\n",
      "attention mask torch.Size([1, 153])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Epoch 210, Average Loss: 4.45223742723465\n",
      "Epoch 220, Average Loss: 4.314719021320343\n",
      "Epoch 230, Average Loss: 4.111126601696014\n",
      "Epoch 240, Average Loss: 4.039401650428772\n",
      "Epoch 250, Average Loss: 3.8485316038131714\n",
      "Epoch 260, Average Loss: 3.5308111906051636\n",
      "Epoch 270, Average Loss: 3.591161370277405\n",
      "Epoch 280, Average Loss: 3.0834324955940247\n",
      "Epoch 290, Average Loss: 3.0782092213630676\n",
      "Epoch 300, Average Loss: 3.387939453125\n"
     ]
    }
   ],
   "source": [
    "DEBUG=True\n",
    "def train_step(batch, model, tokenizer, optimizer):\n",
    "    global DEBUG\n",
    "    # Tokenize the combined student text (prompt + response)\n",
    "    inputs = tokenizer(\n",
    "        batch['combined_student'], \n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Forward pass through student model\n",
    "    student_outputs = model(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        labels=inputs.input_ids,  # For calculating loss\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    \n",
    "    # Get teacher logits from dataset\n",
    "    #note that it is a list of tensors, so we need to stack them\n",
    "    #teacher_logits =  torch.cat(batch['response_logits'], dim=1).to(device)\n",
    "    teacher_logits =  batch['response_logits'] #.to(device)\n",
    "\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"teacher_logits\", teacher_logits.shape)\n",
    "        print(\"student_outputs\", student_outputs.logits.shape)\n",
    "        #also print the decode, we need to apply argmax to get the token, and then decode\n",
    "        #using repr() to show escaped characters\n",
    "        print(\"student decoded\", repr(tokenizer.decode(torch.argmax(student_outputs.logits, dim=-1)[0], skip_special_tokens=False)))\n",
    "        print(\"teacher decoded\", repr(tokenizer.decode(torch.argmax(teacher_logits, dim=-1)[0], skip_special_tokens=False)))\n",
    "        #print the argmax too:\n",
    "        print(\"student argmax\", torch.argmax(student_outputs.logits, dim=-1)[0])\n",
    "        print(\"teacher argmax\", torch.argmax(teacher_logits, dim=-1)[0])\n",
    "        #also decode the input_ids to see if they are correct\n",
    "        print(\"input_ids\", inputs.input_ids)\n",
    "        print(\"input_ids decoded\", repr(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=False)))\n",
    "        print(\"input_ids\", inputs.input_ids.shape)\n",
    "        #print also the attention mask\n",
    "        print(\"attention mask\", inputs.attention_mask.shape)\n",
    "        print(\"attention mask\", inputs.attention_mask)\n",
    "    \n",
    "    # Calculate KL divergence loss between student and teacher logits\n",
    "    # Only consider the logits for generated tokens (not prompt)\n",
    "    #print shapes, to debug:\n",
    "    kl_loss = F.kl_div(\n",
    "        F.log_softmax(student_outputs.logits[:, -teacher_logits.size(1):], dim=-1),\n",
    "        F.softmax(teacher_logits, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    kl_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return kl_loss.item()\n",
    "\n",
    "# Set up optimizer\n",
    "#optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)\n",
    "#set the optimizer only in parameters that require grad, not the ones of the teacher model\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, student_model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in examples:\n",
    "        loss = train_step(batch, student_model, teacher_tokenizer, optimizer)\n",
    "        total_loss += loss\n",
    "    DEBUG=False\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(examples)}\")\n",
    "    if epoch % 100 == 4:\n",
    "        DEBUG=True\n",
    "        #print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(examples)}\")\n",
    "        #print(\"Saving model\")\n",
    "        #student_model.save_adapter_fusion(\"student_model\")\n",
    "        #print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.22.self_attn.o_proj.weight True\n",
      "model.layers.22.mlp.gate_proj.weight True\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight True\n",
      "model.layers.22.post_attention_layernorm.weight True\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.23.self_attn.k_proj.weight True\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.23.self_attn.o_proj.weight True\n",
      "model.layers.23.mlp.gate_proj.weight True\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight True\n",
      "model.layers.23.post_attention_layernorm.weight True\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.24.self_attn.k_proj.weight True\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.24.self_attn.o_proj.weight True\n",
      "model.layers.24.mlp.gate_proj.weight True\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight True\n",
      "model.layers.24.post_attention_layernorm.weight True\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.25.self_attn.o_proj.weight True\n",
      "model.layers.25.mlp.gate_proj.weight True\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight True\n",
      "model.layers.25.post_attention_layernorm.weight True\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.26.self_attn.o_proj.weight True\n",
      "model.layers.26.mlp.gate_proj.weight True\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight True\n",
      "model.layers.26.post_attention_layernorm.weight True\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.27.self_attn.o_proj.weight True\n",
      "model.layers.27.mlp.gate_proj.weight True\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight True\n",
      "model.layers.27.post_attention_layernorm.weight True\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.28.self_attn.k_proj.weight True\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.28.self_attn.o_proj.weight True\n",
      "model.layers.28.mlp.gate_proj.weight True\n",
      "model.layers.28.mlp.up_proj.weight True\n",
      "model.layers.28.mlp.down_proj.weight True\n",
      "model.layers.28.input_layernorm.weight True\n",
      "model.layers.28.post_attention_layernorm.weight True\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.29.self_attn.k_proj.weight True\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.29.self_attn.o_proj.weight True\n",
      "model.layers.29.mlp.gate_proj.weight True\n",
      "model.layers.29.mlp.up_proj.weight True\n",
      "model.layers.29.mlp.down_proj.weight True\n",
      "model.layers.29.input_layernorm.weight True\n",
      "model.layers.29.post_attention_layernorm.weight True\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.30.self_attn.k_proj.weight True\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.30.self_attn.o_proj.weight True\n",
      "model.layers.30.mlp.gate_proj.weight True\n",
      "model.layers.30.mlp.up_proj.weight True\n",
      "model.layers.30.mlp.down_proj.weight True\n",
      "model.layers.30.input_layernorm.weight True\n",
      "model.layers.30.post_attention_layernorm.weight True\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.31.self_attn.k_proj.weight True\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.31.self_attn.o_proj.weight True\n",
      "model.layers.31.mlp.gate_proj.weight True\n",
      "model.layers.31.mlp.up_proj.weight True\n",
      "model.layers.31.mlp.down_proj.weight True\n",
      "model.layers.31.input_layernorm.weight True\n",
      "model.layers.31.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "with student_model.disable_adapter():\n",
    "   student_checksums=compute_model_checksums(student_model.base_model.model)\n",
    "for key in initial_checksums.keys(): \n",
    "    #replace termination .weight with .base_layer.weight\n",
    "    newkey=key.replace('.weight','.base_layer.weight')\n",
    "    #compare with key in student model if exists\n",
    "    if newkey in student_checksums.keys():\n",
    "        print(newkey, initial_checksums[key] == student_checksums[newkey])\n",
    "    elif key in student_checksums.keys():\n",
    "        print(key, initial_checksums[key] == student_checksums[key])\n",
    "    else:\n",
    "        print(f\"Key {newkey} not found in student model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.22.self_attn.o_proj.weight True\n",
      "model.layers.22.mlp.gate_proj.weight True\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight True\n",
      "model.layers.22.post_attention_layernorm.weight True\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.23.self_attn.k_proj.weight True\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.23.self_attn.o_proj.weight True\n",
      "model.layers.23.mlp.gate_proj.weight True\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight True\n",
      "model.layers.23.post_attention_layernorm.weight True\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.24.self_attn.k_proj.weight True\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.24.self_attn.o_proj.weight True\n",
      "model.layers.24.mlp.gate_proj.weight True\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight True\n",
      "model.layers.24.post_attention_layernorm.weight True\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.25.self_attn.o_proj.weight True\n",
      "model.layers.25.mlp.gate_proj.weight True\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight True\n",
      "model.layers.25.post_attention_layernorm.weight True\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.26.self_attn.o_proj.weight True\n",
      "model.layers.26.mlp.gate_proj.weight True\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight True\n",
      "model.layers.26.post_attention_layernorm.weight True\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.27.self_attn.o_proj.weight True\n",
      "model.layers.27.mlp.gate_proj.weight True\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight True\n",
      "model.layers.27.post_attention_layernorm.weight True\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.28.self_attn.k_proj.weight True\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.28.self_attn.o_proj.weight True\n",
      "model.layers.28.mlp.gate_proj.weight True\n",
      "model.layers.28.mlp.up_proj.weight True\n",
      "model.layers.28.mlp.down_proj.weight True\n",
      "model.layers.28.input_layernorm.weight True\n",
      "model.layers.28.post_attention_layernorm.weight True\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.29.self_attn.k_proj.weight True\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.29.self_attn.o_proj.weight True\n",
      "model.layers.29.mlp.gate_proj.weight True\n",
      "model.layers.29.mlp.up_proj.weight True\n",
      "model.layers.29.mlp.down_proj.weight True\n",
      "model.layers.29.input_layernorm.weight True\n",
      "model.layers.29.post_attention_layernorm.weight True\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.30.self_attn.k_proj.weight True\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.30.self_attn.o_proj.weight True\n",
      "model.layers.30.mlp.gate_proj.weight True\n",
      "model.layers.30.mlp.up_proj.weight True\n",
      "model.layers.30.mlp.down_proj.weight True\n",
      "model.layers.30.input_layernorm.weight True\n",
      "model.layers.30.post_attention_layernorm.weight True\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight True\n",
      "model.layers.31.self_attn.k_proj.weight True\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight True\n",
      "model.layers.31.self_attn.o_proj.weight True\n",
      "model.layers.31.mlp.gate_proj.weight True\n",
      "model.layers.31.mlp.up_proj.weight True\n",
      "model.layers.31.mlp.down_proj.weight True\n",
      "model.layers.31.input_layernorm.weight True\n",
      "model.layers.31.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "student_checksums=compute_model_checksums(student_model.base_model.model)\n",
    "for key in initial_checksums.keys(): \n",
    "    #replace termination .weight with .base_layer.weight\n",
    "    newkey=key.replace('.weight','.base_layer.weight')\n",
    "    #compare with key in student model if exists\n",
    "    if newkey in student_checksums.keys():\n",
    "        print(newkey, initial_checksums[key] == student_checksums[newkey])\n",
    "    elif key in student_checksums.keys():\n",
    "        print(key, initial_checksums[key] == student_checksums[key])\n",
    "    else:\n",
    "        print(f\"Key {newkey} not found in student model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 33, 32001]) torch.Size([1, 33]) torch.Size([1, 256]) torch.Size([1, 289]) 33\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain how a binary search works. \\n\\n Your Answer:\\n  Your\\n   \\n  \\n```\\n##\\n#\\n>\\n>>\\n<<\\n//\\n The\\n A\\nA\\na\\n a\\n</s>']\n",
      "Task: Explain how a binary search works.\n",
      "Answer: ['\\n  Your\\n   \\n  \\n```\\n##\\n#\\n>\\n>>\\n<<\\n//\\n The\\n A\\nA\\na\\n a\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 47, 32001]) torch.Size([1, 47]) torch.Size([1, 256]) torch.Size([1, 303]) 47\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: What is the difference between a list and tuple in Python? \\n\\n Your Answer:\\n   The  A  a\\n1\\n```1``````\\n ```\\n  \\n   \\n    \\n##\\n ##\\n ###\\n ####\\n####\\n The\\nThe\\nThere\\nYou\\nyou\\ny\\n</s>']\n",
      "Task: What is the difference between a list and tuple in Python?\n",
      "Answer: ['\\n   The  A  a\\n1\\n```1``````\\n ```\\n  \\n   \\n    \\n##\\n ##\\n ###\\n ####\\n####\\n The\\nThe\\nThere\\nYou\\nyou\\ny\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 112, 32001]) torch.Size([1, 112]) torch.Size([1, 256]) torch.Size([1, 368]) 112\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: How does garbage collection work in Python? \\n\\n Your Answer:\\n   The  Gar  G  C  Python\\n##\\n```\\n   \\n  \\n ```\\n    \\n     \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 2 3 4 5 6 7 8 9  - + * / * *\\n. ⃝  (  )  [  ]  {  While While If If While if If if if while while if of of if\\n of\\n</s>']\n",
      "Task: How does garbage collection work in Python?\n",
      "Answer: ['\\n   The  Gar  G  C  Python\\n##\\n```\\n   \\n  \\n ```\\n    \\n     \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 2 3 4 5 6 7 8 9  - + * / * *\\n. ⃝  (  )  [  ]  {  While While If If While if If if if while while if of of if\\n of\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 21, 32001]) torch.Size([1, 21]) torch.Size([1, 256]) torch.Size([1, 277]) 21\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain the concept of decorators in Python. \\n\\n Your Answer:\\n   The  A  Dec or  in ator  _or\\n```\\n</s>']\n",
      "Task: Explain the concept of decorators in Python.\n",
      "Answer: ['\\n   The  A  Dec or  in ator  _or\\n```\\n</s>']\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#now we test for the tasks... TIENE PINTA DE QIE SE ESTA DESTRUYENDO EL TEACHER MODEL TAMBIEN!!!!\n",
    "print(SYSTEM_PROMPT)\n",
    "for task in tasks:\n",
    "    full_prompt = f\" {SYSTEM_PROMPT}\\n\\nYour Task: {task} \\n\\n Your Answer:\"\n",
    "    student_prompt = f\" \\n\\nTask: {task} \\n\\n Your Answer:\"\n",
    "    #with teacher_model.disable_adapter():\n",
    "    decoded,logits,_=generate_response(teacher_model, teacher_tokenizer, [full_prompt])\n",
    "    print (f\"Task: {task}\")\n",
    "    print (f\"Answer: {decoded}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 15, 32001]) torch.Size([1, 15]) torch.Size([1, 256]) torch.Size([1, 271]) 15\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain how a binary search works. \\n\\n Your Answer:\\n  Binary Binary  Search Search\\n##\\n```\\n</s>']\n",
      "Task: Explain how a binary search works.\n",
      "Answer: ['\\n  Binary Binary  Search Search\\n##\\n```\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 14, 32001]) torch.Size([1, 14]) torch.Size([1, 256]) torch.Size([1, 270]) 14\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: What is the difference between a list and tuple in Python? \\n\\n Your Answer:\\n   The  A  Python\\n   \\n```\\n</s>']\n",
      "Task: What is the difference between a list and tuple in Python?\n",
      "Answer: ['\\n   The  A  Python\\n   \\n```\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 30, 32001]) torch.Size([1, 30]) torch.Size([1, 256]) torch.Size([1, 286]) 30\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: How does garbage collection work in Python? \\n\\n Your Answer:\\n   The  Gar  G  C  Python\\n##\\n```\\n1\\n   \\n  \\nG\\nC\\nGC\\n</s>']\n",
      "Task: How does garbage collection work in Python?\n",
      "Answer: ['\\n   The  Gar  G  C  Python\\n##\\n```\\n1\\n   \\n  \\nG\\nC\\nGC\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 128, 32001]) torch.Size([1, 128]) torch.Size([1, 256]) torch.Size([1, 384]) 128\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain the concept of decorators in Python. \\n\\n Your Answer:\\n   The  Dec  A or   \\n```\\n##\\n   \\n   ```\\n    \\n ```  ``` `````` ````````````` ```    _  __  @  `  (  )  <  >  \" ,  . (,\\n. .\\n..\\n...\\nThe\\nThere\\nYou\\nyou\\nthat\\nit\\nthe\\na\\nan\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\n-\\n+\\n*\\n=\\n==\\n===\\n====\\n========\\n>>>\\n> >\\n']\n",
      "Task: Explain the concept of decorators in Python.\n",
      "Answer: ['\\n   The  Dec  A or   \\n```\\n##\\n   \\n   ```\\n    \\n ```  ``` `````` ````````````` ```    _  __  @  `  (  )  <  >  \" ,  . (,\\n. .\\n..\\n...\\nThe\\nThere\\nYou\\nyou\\nthat\\nit\\nthe\\na\\nan\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\n-\\n+\\n*\\n=\\n==\\n===\\n====\\n========\\n>>>\\n> >\\n']\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#now we test for the tasks\n",
    "print(SYSTEM_PROMPT)\n",
    "for task in tasks:\n",
    "    full_prompt = f\"{SYSTEM_PROMPT}\\n\\nYour Task: {task} \\n\\n Your Answer:\"\n",
    "    student_prompt = f\" \\n\\nTask: {task} \\n\\n Your Answer:\"\n",
    "    decoded,logits,_=generate_response(student_model, teacher_tokenizer, [full_prompt])\n",
    "    print (f\"Task: {task}\")\n",
    "    print (f\"Answer: {decoded}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 15, 32001]) torch.Size([1, 15]) torch.Size([1, 256]) torch.Size([1, 271]) 15\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  \\n\\nTask: Explain how a binary search works. \\n\\n Your Answer:\\n##\\n  Binary  Search\\nSearch\\nThe\\n1\\n</s>']\n",
      "Task: Explain how a binary search works.\n",
      "Answer: ['\\n##\\n  Binary  Search\\nSearch\\nThe\\n1\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 54, 32001]) torch.Size([1, 54]) torch.Size([1, 256]) torch.Size([1, 310]) 54\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  \\n\\nTask: What is the difference between a list and tuple in Python? \\n\\n Your Answer:\\n   The  There  It  They       \\n           \\n```\\n##\\nThe\\nThere\\nThey\\nYou\\n You\\n   You     It   Tu Tu T Tu  Tu   T  T\\nA\\n</s>']\n",
      "Task: What is the difference between a list and tuple in Python?\n",
      "Answer: ['\\n   The  There  It  They       \\n           \\n```\\n##\\nThe\\nThere\\nThey\\nYou\\n You\\n   You     It   Tu Tu T Tu  Tu   T  T\\nA\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 33, 32001]) torch.Size([1, 33]) torch.Size([1, 256]) torch.Size([1, 289]) 33\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  \\n\\nTask: How does garbage collection work in Python? \\n\\n Your Answer:\\n   The  Gar  Python\\n##\\n   \\n```\\nGar\\nPython\\n Python  In  This  a  the  You\\n</s>']\n",
      "Task: How does garbage collection work in Python?\n",
      "Answer: ['\\n   The  Gar  Python\\n##\\n   \\n```\\nGar\\nPython\\n Python  In  This  a  the  You\\n</s>']\n",
      "=====================================\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 19, 32001]) torch.Size([1, 19]) torch.Size([1, 256]) torch.Size([1, 275]) 19\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s>  \\n\\nTask: Explain the concept of decorators in Python. \\n\\n Your Answer:\\nA\\n```Python\\nPython  Python  Dec Python Python Dec Dec\\n##\\n</s>']\n",
      "Task: Explain the concept of decorators in Python.\n",
      "Answer: ['\\nA\\n```Python\\nPython  Python  Dec Python Python Dec Dec\\n##\\n</s>']\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#now we test for the tasks\n",
    "for task in tasks:\n",
    "    full_prompt = f\"{SYSTEM_PROMPT}\\n\\nYour Task: {task} \\n\\n Your Answer:\"\n",
    "    student_prompt = f\" \\n\\nTask: {task} \\n\\n Your Answer:\"\n",
    "    decoded,logits,_=generate_response(student_model, teacher_tokenizer, [student_prompt])\n",
    "    print (f\"Task: {task}\")\n",
    "    print (f\"Answer: {decoded}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: Explain how a binary search works. \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 15, 32001]) torch.Size([1, 15]) torch.Size([1, 256]) torch.Size([1, 271]) 15\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain how a binary search works. \\n\\n Your Answer:\\n  Binary Binary  Search Search\\n##\\n```\\n</s>']\n",
      "Teacher response: \n",
      "['\\n  Binary Binary  Search Search\\n##\\n```\\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 15, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 109\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: What is the difference between a list and tuple in Python? \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 14, 32001]) torch.Size([1, 14]) torch.Size([1, 256]) torch.Size([1, 270]) 14\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: What is the difference between a list and tuple in Python? \\n\\n Your Answer:\\n   The  A  Python\\n   \\n```\\n</s>']\n",
      "Teacher response: \n",
      "['\\n   The  A  Python\\n   \\n```\\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 14, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 121\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: How does garbage collection work in Python? \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 116, 32001]) torch.Size([1, 116]) torch.Size([1, 256]) torch.Size([1, 372]) 116\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: How does garbage collection work in Python? \\n\\n Your Answer:\\n   The  Gar  G  C  Python\\n##\\n```\\n1\\n   \\n  \\n ```\\n    \\n     \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 2 3 4 5 6 7 8 9  - + * / * *\\n. * ⃝ \\u2003 \\u2002 \\u2009 \\u3000  ````````` ``` ```  “ “ \" \"\\nThe\\n The\\n A\\n a\\na\\n</s>']\n",
      "Teacher response: \n",
      "['\\n   The  Gar  G  C  Python\\n##\\n```\\n1\\n   \\n  \\n ```\\n    \\n     \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 2 3 4 5 6 7 8 9  - + * / * *\\n. * ⃝ \\u2003 \\u2002 \\u2009 \\u3000  ````````` ``` ```  “ “ \" \"\\nThe\\n The\\n A\\n a\\na\\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 116, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 373\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: Explain the concept of decorators in Python. \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 103, 32001]) torch.Size([1, 103]) torch.Size([1, 256]) torch.Size([1, 359]) 103\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain the concept of decorators in Python. \\n\\n Your Answer:\\n   The  Dec  A or  In  Python\\n```\\n##\\n   \\n  \\n ```\\n    \\n     \\n      \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 2 3 4 5 6 7 8 9  - + * / * *\\n. *. .\\n..\\n...\\nThe\\n The\\n There\\nThere\\nthere\\n there\\n</s>']\n",
      "Teacher response: \n",
      "['\\n   The  Dec  A or  In  Python\\n```\\n##\\n   \\n  \\n ```\\n    \\n     \\n      \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 2 3 4 5 6 7 8 9  - + * / * *\\n. *. .\\n..\\n...\\nThe\\n The\\n There\\nThere\\nthere\\n there\\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 103, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 356\n"
     ]
    }
   ],
   "source": [
    "# Define system prompt and tasks\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
    "Always format code properly and explain technical concepts clearly.\"\"\"\n",
    "\n",
    "tasks = [\n",
    "    \"Explain how a binary search works.\",\n",
    "    \"What is the difference between a list and tuple in Python?\",\n",
    "    \"How does garbage collection work in Python?\",\n",
    "    \"Explain the concept of decorators in Python.\",\n",
    "]\n",
    "def create_training_examples():\n",
    "    examples = []\n",
    "    for task in tasks:\n",
    "        full_prompt = f\"{SYSTEM_PROMPT}\\n\\nYour Task: {task} \\n\\n Your Answer:\"\n",
    "        student_prompt = f\"\\n\\nTask: {task} \\n\\n Your Answer:\"\n",
    "        # Get teacher's response\n",
    "        print(f\"Teacher prompt: \\n{full_prompt}\")\n",
    "        #with teacher_model.disable_adapter():\n",
    "        teacher_response, new_logits, _ = generate_response(teacher_model, teacher_tokenizer, full_prompt)\n",
    "        print (f\"Teacher response: \\n{teacher_response}\")\n",
    "        print(\"=====================================\")\n",
    "        examples.append({\n",
    "            \"prompt\": full_prompt,\n",
    "            \"student_prompt\": f\"\\n\\nTask: {task} \\n\\n Your Answer:\",\n",
    "            \"response_logits\": new_logits,\n",
    "            \"combined\": f\"{full_prompt}{teacher_response}\",\n",
    "            \"combined_student\": f\"{student_prompt}{teacher_response}\"\n",
    "        })\n",
    "        print(\"shape of new_logits\", new_logits.shape)\n",
    "        print(\"len of teacher response\", len(teacher_response))\n",
    "        print(\"len of combined_student\", len(f\"{student_prompt}{teacher_response}\"))\n",
    "    return examples\n",
    "\n",
    "examples= create_training_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: Explain how a binary search works. \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 15, 32001]) torch.Size([1, 15]) torch.Size([1, 256]) torch.Size([1, 271]) 15\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain how a binary search works. \\n\\n Your Answer:\\n  Binary Binary  Search Search\\n##\\n```\\n</s>']\n",
      "Teacher response: \n",
      "['\\n  Binary Binary  Search Search\\n##\\n```\\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 15, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 109\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: What is the difference between a list and tuple in Python? \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 14, 32001]) torch.Size([1, 14]) torch.Size([1, 256]) torch.Size([1, 270]) 14\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: What is the difference between a list and tuple in Python? \\n\\n Your Answer:\\n   The  A  a   \\n```\\n</s>']\n",
      "Teacher response: \n",
      "['\\n   The  A  a   \\n```\\n</s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 14, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 114\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: How does garbage collection work in Python? \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 73, 32001]) torch.Size([1, 73]) torch.Size([1, 256]) torch.Size([1, 329]) 73\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: How does garbage collection work in Python? \\n\\n Your Answer:\\n   The  Gar  G  C  Python\\n##\\n```\\n1\\n   \\n  \\n ```\\n    \\n     \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 9 8 7 6 5 4 3 2 </s>']\n",
      "Teacher response: \n",
      "['\\n   The  Gar  G  C  Python\\n##\\n```\\n1\\n   \\n  \\n ```\\n    \\n     \\n       \\n        \\n          \\n           \\n            \\n              \\n                                    1 0 9 8 7 6 5 4 3 2 </s>']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 73, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 274\n",
      "Teacher prompt: \n",
      "You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
      "Always format code properly and explain technical concepts clearly.\n",
      "\n",
      "Your Task: Explain the concept of decorators in Python. \n",
      "\n",
      " Your Answer:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 128, 32001]) torch.Size([1, 128]) torch.Size([1, 256]) torch.Size([1, 384]) 128\n",
      "['[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> You are a helpful AI assistant that provides clear, accurate, and concise answers.\\nAlways format code properly and explain technical concepts clearly.\\n\\nYour Task: Explain the concept of decorators in Python. \\n\\n Your Answer:\\n   The  Dec  A or   \\n```\\n##\\n   \\n   ```\\n    \\n ```  ``` `````` ````````````` ```    _  __  @  `  (  )  <  >  \" ,  . (,\\n. .\\n..\\n...\\nThe\\nThere\\nYou\\nA\\na\\nan\\nin\\nis\\nare\\nbe\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\nk\\nl\\nm\\nn\\no\\np\\nq\\nr\\ns\\nt\\nu\\n']\n",
      "Teacher response: \n",
      "['\\n   The  Dec  A or   \\n```\\n##\\n   \\n   ```\\n    \\n ```  ``` `````` ````````````` ```    _  __  @  `  (  )  <  >  \" ,  . (,\\n. .\\n..\\n...\\nThe\\nThere\\nYou\\nA\\na\\nan\\nin\\nis\\nare\\nbe\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\nk\\nl\\nm\\nn\\no\\np\\nq\\nr\\ns\\nt\\nu\\n']\n",
      "=====================================\n",
      "shape of new_logits torch.Size([1, 128, 32001])\n",
      "len of teacher response 1\n",
      "len of combined_student 316\n"
     ]
    }
   ],
   "source": [
    "# Define system prompt and tasks\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that provides clear, accurate, and concise answers.\n",
    "Always format code properly and explain technical concepts clearly.\"\"\"\n",
    "\n",
    "tasks = [\n",
    "    \"Explain how a binary search works.\",\n",
    "    \"What is the difference between a list and tuple in Python?\",\n",
    "    \"How does garbage collection work in Python?\",\n",
    "    \"Explain the concept of decorators in Python.\",\n",
    "]\n",
    "def create_training_examples():\n",
    "    examples = []\n",
    "    for task in tasks:\n",
    "        full_prompt = f\"{SYSTEM_PROMPT}\\n\\nYour Task: {task} \\n\\n Your Answer:\"\n",
    "        student_prompt = f\"\\n\\nTask: {task} \\n\\n Your Answer:\"\n",
    "        # Get teacher's response\n",
    "        print(f\"Teacher prompt: \\n{full_prompt}\")\n",
    "        #with teacher_model.disable_adapter():\n",
    "        teacher_response, new_logits, _ = generate_response(teacher_model, teacher_tokenizer, full_prompt)\n",
    "        print (f\"Teacher response: \\n{teacher_response}\")\n",
    "        print(\"=====================================\")\n",
    "        examples.append({\n",
    "            \"prompt\": full_prompt,\n",
    "            \"student_prompt\": f\"\\n\\nTask: {task} \\n\\n Your Answer:\",\n",
    "            \"response_logits\": new_logits,\n",
    "            \"combined\": f\"{full_prompt}{teacher_response}\",\n",
    "            \"combined_student\": f\"{student_prompt}{teacher_response}\"\n",
    "        })\n",
    "        print(\"shape of new_logits\", new_logits.shape)\n",
    "        print(\"len of teacher response\", len(teacher_response))\n",
    "        print(\"len of combined_student\", len(f\"{student_prompt}{teacher_response}\"))\n",
    "    return examples\n",
    "\n",
    "examples= create_training_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://huggingface.co/docs/transformers/v4.48.0/en/model_doc/llama#transformers.LlamaForCausalLM for more information on the LlamaForCausalLM model.\n",
    "\n",
    "It seems to implement left-censoring incorrectly. The result is different if one proceeds token to token, compared to a complete phrase.\n",
    "\n",
    "Parte de la culpa es del tokenizador, que en modo fast escoje diferentes tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
